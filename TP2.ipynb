{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP2:\n",
    "\n",
    "Au cours du TP1, nous avons étudié le modèle *Softmax* pour traiter le problème de classification probabiliste. Le but était de présenter deux étapes importantes de l'entraînement : la forward propagation et la mise à jour des paramètres. Le TP2 reprend le modèle Softmax dans un cadre plus général, celui des réseaux de neurones avec couches cachèes.\n",
    "Dans ce cadre, on peut considérer le modèle Softmax comme un \"module\" qui prend en entrèe des \"features\", e.g. les pixels d'une image, et qui donne en sortie une loi de probabilité sur les étiquettes.\n",
    "Un réseau de neurones est composé de plusieurs modules, transformant simplement les features d'un espace à un autre en fonction des valeurs courantes des paramètres. Ainsi, le but de l'entraînement est d'apprendre les transformations pertinentes, i.e., en modifiant les paramètres, qui permettront de réaliser la tâche associée au module de sortie. En augmentant le nombre de modules (mais aussi de fonctions non-linéaires), on augmente ainsi la complexité du modèle.\n",
    "\n",
    "Le premier but du TP2 est de programmer les trois étapes essentielles à l'entraînement d'un réseau de neurones : la *forward propagation*, la *backpropagation* et la *mise à jour des paramètres*. Vérifiez que votre modèle fonctionne. Ensuite, vous pourrez comparer les performances de votre réseau de neurones avec celles de votre modèle Softmax de la semaine dernière.\n",
    "\n",
    "Une fois ces bases réalisées, on va pouvoir ajouter plusieurs fonctions d'activations en plus de la classique *sigmoïde*: les *tanh* et *relu*. Vous pourrez comparer la sigmoïde et la tanh avec la relu, notamment lorsque l'on utilise 2 couches cachées ou plus. Vous pourrez aussi mettre en évidence le phénomène de sur-apprentissage (travaillez avec une petite sous partie des données si nécessaire).\n",
    "Pour rappel, les fonctions sont:\n",
    "\n",
    "$$ tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n",
    "\n",
    "$$ relu(x) = max(0, x) $$\n",
    "\n",
    "Remarque: La fonction relu est plus instable numériquement que les deux autres. Il est possible qu'il soit nécessaire de réduire le taux d'apprentissage (ou de l'apapter, en le réduisant au fur et à mesure que l'apprentissage progresse) ou de forcer les valeurs de la relu à rester en dessous d'une limite que l'on choisit (*clipping*).\n",
    "\n",
    "Enfin, on va implémenter la *régularisation*: on ajoutera à la fonction de mise à jour des paramètres les méthodes de régularisation *L1* et *L2*. Il s'agira ensuite de vérifier leur influence sur les courbes d'apprentissage en faisant varier le paramètre $\\lambda$.\n",
    "\n",
    "A faire: \n",
    "- Compléter les fonctions:\n",
    "    - getDimDataset\n",
    "    - sigmoid\n",
    "    - forward\n",
    "    - backward\n",
    "    - update\n",
    "    - softmax\n",
    "    - computeLoss\n",
    "    - getMiniBatch\n",
    "- Compléter les fonctions:\n",
    "    - tanh\n",
    "    - relu\n",
    "    - et faire les expériences demandées.\n",
    "- Compléter les fonctions:\n",
    "    - updateParams\n",
    "    - et faire les expériences demandées.\n",
    "- Envoyer le notebook avec le code complété avant le **13 décembre 2017** à l'adresse **labeau@limsi.fr** accompagné d'un résumé d'un maximum de 6 pages contenant des figures et des analyses rapides des expériences demandées.\n",
    "- Le résumé doit être succinct et se focaliser uniquement sur les points essentiels reliés à l'entraînement des réseaux de neurones. En plus des résultats, ce document doit décrire les difficultés que vous avez rencontrées et, dans le cas échéant, les solutions utilisées pour les résoudre. Vous pouvez aussi y décrire vos questions ouvertes et proposer une expérience sur MNIST afin d'y répondre.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784, 10)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    !wget http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "\n",
    "#####################\n",
    "# Gestion des données\n",
    "#####################  \n",
    "import dataset_loader\n",
    "train_set, valid_set, test_set = dataset_loader.load_mnist()\n",
    "\n",
    "def getDimDataset(train_set):\n",
    "    n_training = len(train_set[0])\n",
    "    n_feature = len(train_set[0][0])\n",
    "    n_label = len(set(train_set[1]))\n",
    "    return n_training, n_feature, n_label\n",
    "\n",
    "n_training, n_feature, n_label = getDimDataset(train_set)\n",
    "print(n_training,n_feature,n_label)\n",
    "\n",
    "########################\n",
    "# Gestion des paramètres\n",
    "########################\n",
    "\n",
    "# Taille de la couche cachée: sous forme de liste, il est possible\n",
    "# d'utiliser plusieurs couches cachées, avec par exemple [128, 64]\n",
    "n_hidden =[100]\n",
    "\n",
    "# Fonction d'activation: à choisir parmi 'sigmoid', 'tanh' et 'relu'\n",
    "acti_fun = 'relu'\n",
    "act_func = 'relu'\n",
    "\n",
    "# Taille du batch\n",
    "batch_size = 500\n",
    "\n",
    "# Taux d'apprentissage:\n",
    "eta = 0.001\n",
    "\n",
    "# Nombre d'époques:\n",
    "n_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMiniBatch(i, batch_size, train_set, one_hot):\n",
    "    \"\"\"\n",
    "    Return a minibatch from the training set and the associated labels\n",
    "    Inputs: i: the identifier of the minibatch - int\n",
    "          : batch_size: the number of training examples - int\n",
    "          : train_set: the training set - ndarray\n",
    "          : one_hot: the one-hot representation of the labels - ndarray\n",
    "    Outputs: the minibatch of examples - ndarray\n",
    "           : the minibatch of labels - ndarray\n",
    "           : the number of examples in the minibatch - int\n",
    "    \"\"\"\n",
    "    idx_begin = i\n",
    "    idx_end = i+batch_size\n",
    "    batch = train_set[0][idx_begin:idx_end]\n",
    "    one_hot=one_hot.transpose()\n",
    "    one_hot_batch = one_hot[idx_begin:idx_end,:]\n",
    "    mini_batch_size = batch.shape[0]\n",
    "    return np.asfortranarray(batch), one_hot_batch, mini_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initNetwork(nn_arch, act_func_name):\n",
    "    \"\"\"\n",
    "    Initialize the neural network weights, activation function and return the number of parameters\n",
    "    Inputs: nn_arch: the number of units per hidden layer -  list of int\n",
    "          : act_func_name: the activation function name (sigmoid, tanh or relu) - str\n",
    "    Outputs: W: a list of weights for each hidden layer - list of ndarray\n",
    "           : B: a list of bias for each hidden layer - list of ndarray\n",
    "           : act_func: the activation function - function\n",
    "           : nb_params: the number of parameters  - int\n",
    "    \"\"\"\n",
    "    W,B = [],[]\n",
    "    sigma = 1.0\n",
    "    act_func = globals()[act_func_name] # Cast the string to a function\n",
    "    nb_params = 0\n",
    "\n",
    "    if act_func_name=='sigmoid':\n",
    "        sigma = 4.0\n",
    "\n",
    "    for i in range(np.size(nn_arch)-1):\n",
    "        w = np.random.normal(loc=0.0, scale=sigma/np.sqrt(nn_arch[i]), size=(nn_arch[i+1],nn_arch[i]))\n",
    "        W.append(w)\n",
    "        b = np.zeros((w.shape[0],1))\n",
    "        if act_func_name=='sigmoid':\n",
    "            b = np.sum(w,1).reshape(-1,1)/-2.0\n",
    "        B.append(b)\n",
    "        nb_params += nn_arch[i+1] * nn_arch[i] + nn_arch[i+1]\n",
    "\n",
    "    return W,B,act_func,nb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Fonctions d'activation\n",
    "########################\n",
    "\n",
    "def sigmoid(z, grad_flag=True):\n",
    "    \"\"\"\n",
    "    Perform the sigmoid transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - ndarray\n",
    "          : grad_flag: flag for computing the derivatives w.r.t. z - boolean\n",
    "    Outputs: y: the activation values - ndarray\n",
    "           : yp: the derivatives w.r.t. z - ndarray\n",
    "    \"\"\"\n",
    "    y=1.0/(1+np.exp(-1*z))\n",
    "    yp=y*(1-y)\n",
    "    return y, yp\n",
    "\n",
    "\n",
    "def tanh(z, grad_flag=True):\n",
    "    \"\"\"\n",
    "    Perform the tanh transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - ndarray\n",
    "          : grad_flag: flag for computing the derivatives w.r.t. z - boolean\n",
    "    Outputs: y: the activation values - ndarray\n",
    "    \"\"\"\n",
    "    y=(np.exp(z)-np.exp(-1*z))/(np.exp(z)+np.exp(-1*z))\n",
    "    yp=1-np.power(y,2)\n",
    "    return y, yp\n",
    "\n",
    "\n",
    "def relu(z, grad_flag=True):\n",
    "    \"\"\"\n",
    "    Perform the relu transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - ndarray\n",
    "          : grad_flag: flag for computing the derivatives w.r.t. z - boolean\n",
    "    Outputs: y: the activation values - ndarray\n",
    "    \"\"\"\n",
    "    y=np.maximum(0,z)\n",
    "    yp=np.zeros_like(y)\n",
    "    yp[y>0]=1\n",
    "    return y, yp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Création du réseau\n",
    "####################\n",
    "\n",
    "### Network Architecture\n",
    "nn_arch = np.array([n_feature] + n_hidden + [n_label])\n",
    "\n",
    "### Create the neural network\n",
    "W, B, act_func, nb_params = initNetwork(nn_arch, act_func)\n",
    "#for w in W:\n",
    "#        print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward(act_func, W, B, X):\n",
    "    \"\"\"\n",
    "    Perform the forward propagation\n",
    "    Inputs: act_func: the activation function - function\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias - list of ndarray\n",
    "          : X: the batch - ndarray\n",
    "    Outputs: Y: a list of activation values - list of ndarray\n",
    "           : Yp: a list of the derivatives w.r.t. the pre-activation of the activation values - list of ndarray\n",
    "    \"\"\"\n",
    "    a=X\n",
    "    Yp=[]\n",
    "    Y=[a]\n",
    "    for i in range(len(W)-1):\n",
    "        z=(a.dot(W[i].transpose()))+B[i].transpose()\n",
    "        y,yp=act_func(z)\n",
    "        Y.append(y)\n",
    "        Yp.append(yp)\n",
    "        a=y\n",
    "    \n",
    "    z=(a.dot(W[-1].transpose()))+B[-1].transpose()\n",
    "    Y.append(z)    \n",
    "    return Y, Yp\n",
    "#W, B, act_func, nb_params = initNetwork(nn_arch, \"sigmoid\")\n",
    "#batch=train_set[0][0:5]\n",
    "#one_hot = np.zeros((n_label,n_training))\n",
    "#one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "#batch,one_hot_batch, mini_batch_size=getMiniBatch(0, 5, train_set, one_hot)\n",
    "#Y, Yp=forward(act_func, W, B, batch)\n",
    "#print([y.shape for y in Y])\n",
    "#print([y.shape for y in Yp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "        Perform the softmax transformation to the pre-activation values\n",
    "        :param z: the pre-activation values\n",
    "        :type z: ndarray\n",
    "        :return: the activation values\n",
    "        :rtype: ndarray\n",
    "    \"\"\"\n",
    "    exps = np.exp(z-np.max(z,axis=1).reshape(-1,1))\n",
    "    somme_exps = np.sum(exps,axis=1)\n",
    "    for i in range(somme_exps.shape[0]):\n",
    "        exps[i]/=somme_exps[i]\n",
    "    return exps\n",
    "#out=softmax(z)\n",
    "#print(out.shape)\n",
    "#print (out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def backward(error, W, Yp): \n",
    "    \"\"\"\n",
    "    Perform the backward propagation\n",
    "    Inputs: error: the gradient w.r.t. to the last layer - ndarray\n",
    "          : W: the weights - list of ndarray\n",
    "          : Yp: the derivatives w.r.t. the pre-activation of the activation functions - list of ndarray\n",
    "    Outputs: gradb: a list of gradient w.r.t. the pre-activation with this order [gradb_layer1, ..., error] - list of ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    gradB = [error]\n",
    "    for w, yp in zip(reversed(W), reversed(Yp)):\n",
    "        error = error.dot(w) * yp\n",
    "        gradB.append(error)\n",
    "    gradB.reverse()\n",
    "    return gradB\n",
    "\n",
    "#out = softmax(Y[-1])\n",
    "### Compute the gradient at the top layer\n",
    "#derror = out - one_hot_batch\n",
    "#gradB=backward(derror, W, Yp)\n",
    "#for b in gradB: print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateParams(theta, dtheta, eta, regularizer=None, lamda=0.):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: theta: the network parameters - ndarray w\n",
    "          : dtheta: the updates of the parameters - ndarray\n",
    "          : eta: the step-size of the gradient descent - float\n",
    "          : regularizer: choice of the regularizer: None, 'L1', or 'L2'\n",
    "          : lambda: hyperparamater giving the importance of the regularizer - float\n",
    "    Outputs: the parameters updated - ndarray\n",
    "    \"\"\"\n",
    "    theta=np.copy(theta)\n",
    "    if regularizer=='L1':\n",
    "        theta+=lamda*np.abs(theta)\n",
    "    elif regularizer=='L2':\n",
    "        theta+=lamda*np.power(theta,2)\n",
    "    return theta - eta * dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(eta, batch_size, W, B, gradB, Y, regularizer, lamda):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: eta: the step-size of the gradient descent - float \n",
    "          : batch_size: number of examples in the batch (for normalizing) - int\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias -  list of ndarray\n",
    "          : gradB: the gradient of the activations w.r.t. to the loss -  list of ndarray\n",
    "          : Y: the activation values -  list of ndarray\n",
    "    Outputs: W: the weights updated -  list of ndarray\n",
    "           : B: the bias updated -  list of ndarray\n",
    "    \"\"\"\n",
    "    # grad_b should be a vector: object.reshape(-1,1) can be useful\n",
    "    \n",
    "   #accumulated_gradients_W = [np.zeros_like(w) for w in W]\n",
    "    #for i in range(len(W)):\n",
    "    #    for x in range(batch_size):\n",
    "    #        accumulated_gradients_W[i] += (gradB[i][x].reshape(-1,1)).transpose().dot(Y[i+1][x])\n",
    "    #    W[i]=updateParams(W[i], accumulated_gradients_W[i]/batch_size, eta,regularizer,lamda)\n",
    "    #    B[i]=updateParams(B[i], np.sum(gradB[i], axis=0, keepdims=True).transpose()/batch_size, eta,regularizer,lamda)\n",
    "\n",
    "    #return W, B#\n",
    "    k=len(W)-1\n",
    "    while(k>0):\n",
    "        grad_w=((gradB[k].transpose()).dot(Y[k]))/batch_size\n",
    "        W[k]=(1-eta * lamda/batch_size)*W[k]\n",
    "        W[k]=updateParams(W[k],grad_w, eta,regularizer,lamda)\n",
    "        grad_b = (np.sum(gradB[k],1).reshape(-1,1))/batch_size\n",
    "        B[k]=updateParams(B[k],grad_b[0], eta,regularizer,lamda)\n",
    "        k-=1\n",
    "        \n",
    "    return W,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeLoss(act_func, W, B, X, labels):\n",
    "    \"\"\"\n",
    "    Compute the loss value of the current network on the full batch\n",
    "    Inputs: act_func: the activation function - function\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias - list of ndarray\n",
    "          : X: the batch - ndarray\n",
    "          : labels: the labels corresponding to the batch\n",
    "    Outputs: loss: the negative log-likelihood - float\n",
    "           : accuracy: the ratio of examples that are well-classified - float\n",
    "    \"\"\" \n",
    "    ### Forward propagation\n",
    "    Y, Yp = forward(act_func, W, B, X)\n",
    " \n",
    "    ### Compute the softmax and the prediction\n",
    "    out = softmax(Y[-1])\n",
    "    #pred = np.argmax(out,axis=0)\n",
    "    \n",
    "    #Loss and Accuracy    \n",
    "    loss = -np.sum(np.log(out[np.arange(out.shape[0]), labels]))/labels.shape[0]\n",
    "    accuracy = np.mean((np.argmax(out, axis=1) == labels).astype(np.float, copy=False))\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture = [784 100  10] | Batch size = 500 | Eta = 0.001 | Act Func=relu\n",
      "01/100 0.95s Tloss=2.339 Taccu=11.0% Vloss=2.34 Vaccu=10.71% Eta=0.0012\n",
      "02/100 1.70s Tloss=2.331 Taccu=11.37% Vloss=2.332 Vaccu=11.02% Eta=0.00144\n",
      "03/100 2.43s Tloss=2.321 Taccu=11.93% Vloss=2.322 Vaccu=11.61% Eta=0.001728\n",
      "04/100 3.12s Tloss=2.31 Taccu=12.61% Vloss=2.31 Vaccu=12.48% Eta=0.0020736\n",
      "05/100 3.82s Tloss=2.297 Taccu=13.55% Vloss=2.297 Vaccu=13.44% Eta=0.00248832\n",
      "06/100 4.50s Tloss=2.281 Taccu=14.73% Vloss=2.281 Vaccu=14.51% Eta=0.002985984\n",
      "07/100 5.20s Tloss=2.264 Taccu=16.68% Vloss=2.264 Vaccu=16.26% Eta=0.0035831808\n",
      "08/100 5.93s Tloss=2.244 Taccu=19.43% Vloss=2.243 Vaccu=19.04% Eta=0.00429981696\n",
      "09/100 6.62s Tloss=2.221 Taccu=23.36% Vloss=2.22 Vaccu=22.95% Eta=0.005159780352\n",
      "10/100 7.31s Tloss=2.195 Taccu=27.94% Vloss=2.193 Vaccu=27.89% Eta=0.0061917364224\n",
      "11/100 8.03s Tloss=2.165 Taccu=32.95% Vloss=2.163 Vaccu=33.04% Eta=0.00743008370688\n",
      "12/100 8.74s Tloss=2.131 Taccu=37.68% Vloss=2.128 Vaccu=37.69% Eta=0.00891610044826\n",
      "13/100 9.45s Tloss=2.092 Taccu=41.93% Vloss=2.088 Vaccu=42.02% Eta=0.0106993205379\n",
      "14/100 10.14s Tloss=2.047 Taccu=45.54% Vloss=2.042 Vaccu=45.91% Eta=0.0128391846455\n",
      "15/100 10.84s Tloss=1.996 Taccu=49.01% Vloss=1.99 Vaccu=49.44% Eta=0.0154070215746\n",
      "16/100 11.56s Tloss=1.939 Taccu=52.48% Vloss=1.932 Vaccu=53.43% Eta=0.0184884258895\n",
      "17/100 12.27s Tloss=1.875 Taccu=55.93% Vloss=1.867 Vaccu=57.26% Eta=0.0221861110674\n",
      "18/100 12.97s Tloss=1.805 Taccu=59.06% Vloss=1.795 Vaccu=60.75% Eta=0.0266233332809\n",
      "19/100 13.67s Tloss=1.729 Taccu=61.82% Vloss=1.718 Vaccu=63.65% Eta=0.0319479999371\n",
      "20/100 14.37s Tloss=1.648 Taccu=64.27% Vloss=1.636 Vaccu=66.47% Eta=0.0383375999245\n",
      "21/100 15.07s Tloss=1.565 Taccu=66.37% Vloss=1.55 Vaccu=68.76% Eta=0.0460051199094\n",
      "22/100 15.78s Tloss=1.479 Taccu=68.15% Vloss=1.462 Vaccu=70.5% Eta=0.0552061438912\n",
      "23/100 16.50s Tloss=1.394 Taccu=69.56% Vloss=1.374 Vaccu=71.93% Eta=0.0662473726695\n",
      "24/100 17.22s Tloss=1.311 Taccu=70.72% Vloss=1.288 Vaccu=73.1% Eta=0.0794968472034\n",
      "25/100 17.94s Tloss=1.231 Taccu=71.77% Vloss=1.206 Vaccu=74.31% Eta=0.0953962166441\n",
      "26/100 18.63s Tloss=1.156 Taccu=72.71% Vloss=1.128 Vaccu=75.11% Eta=0.114475459973\n",
      "27/100 19.36s Tloss=1.087 Taccu=73.7% Vloss=1.055 Vaccu=76.04% Eta=0.137370551967\n",
      "28/100 20.08s Tloss=1.023 Taccu=74.46% Vloss=0.9891 Vaccu=76.89% Eta=0.164844662361\n",
      "29/100 20.78s Tloss=0.9659 Taccu=75.19% Vloss=0.9293 Vaccu=77.78% Eta=0.197813594833\n",
      "30/100 21.49s Tloss=0.9147 Taccu=75.88% Vloss=0.8759 Vaccu=78.49% Eta=0.2373763138\n",
      "31/100 22.19s Tloss=0.8695 Taccu=76.48% Vloss=0.8287 Vaccu=79.02% Eta=0.28485157656\n",
      "32/100 22.89s Tloss=0.8301 Taccu=77.0% Vloss=0.7873 Vaccu=79.5% Eta=0.341821891872\n",
      "33/100 23.57s Tloss=0.7959 Taccu=77.5% Vloss=0.7516 Vaccu=79.83% Eta=0.410186270246\n",
      "34/100 24.27s Tloss=0.7668 Taccu=77.89% Vloss=0.721 Vaccu=80.2% Eta=0.492223524295\n",
      "35/100 24.98s Tloss=0.7423 Taccu=78.23% Vloss=0.6951 Vaccu=80.45% Eta=0.590668229154\n",
      "36/100 25.69s Tloss=0.722 Taccu=78.39% Vloss=0.6736 Vaccu=80.64% Eta=0.708801874985\n",
      "37/100 26.39s Tloss=0.7057 Taccu=78.51% Vloss=0.656 Vaccu=80.68% Eta=0.850562249982\n",
      "38/100 27.23s Tloss=0.6929 Taccu=78.59% Vloss=0.642 Vaccu=80.72% Eta=1.02067469998\n",
      "39/100 27.92s Tloss=0.6835 Taccu=78.72% Vloss=0.6313 Vaccu=80.81% Eta=1.22480963997\n",
      "40/100 28.81s Tloss=0.6771 Taccu=78.75% Vloss=0.6236 Vaccu=80.92% Eta=1.46977156797\n",
      "41/100 29.60s Tloss=0.6737 Taccu=78.76% Vloss=0.6188 Vaccu=80.8% Eta=1.76372588156\n",
      "42/100 30.33s Tloss=0.6731 Taccu=78.66% Vloss=0.6168 Vaccu=80.72% Eta=2.11647105788\n",
      "43/100 31.05s Tloss=0.6751 Taccu=78.67% Vloss=0.6175 Vaccu=80.7% Eta=1.6931768463\n",
      "44/100 31.73s Tloss=0.6745 Taccu=78.67% Vloss=0.6172 Vaccu=80.69% Eta=2.03181221556\n",
      "45/100 32.43s Tloss=0.6779 Taccu=78.69% Vloss=0.6194 Vaccu=80.58% Eta=1.62544977245\n",
      "46/100 33.13s Tloss=0.6771 Taccu=78.69% Vloss=0.6188 Vaccu=80.64% Eta=1.95053972694\n",
      "47/100 33.83s Tloss=0.6812 Taccu=78.67% Vloss=0.622 Vaccu=80.46% Eta=1.56043178155\n",
      "48/100 34.53s Tloss=0.6803 Taccu=78.68% Vloss=0.6212 Vaccu=80.49% Eta=1.87251813786\n",
      "49/100 35.23s Tloss=0.6848 Taccu=78.6% Vloss=0.6248 Vaccu=80.35% Eta=1.49801451029\n",
      "50/100 36.00s Tloss=0.6839 Taccu=78.61% Vloss=0.624 Vaccu=80.42% Eta=1.79761741235\n",
      "51/100 36.67s Tloss=0.6885 Taccu=78.59% Vloss=0.6279 Vaccu=80.31% Eta=1.43809392988\n",
      "52/100 37.34s Tloss=0.6875 Taccu=78.6% Vloss=0.627 Vaccu=80.29% Eta=1.72571271585\n",
      "53/100 38.04s Tloss=0.6921 Taccu=78.53% Vloss=0.6309 Vaccu=80.24% Eta=1.38057017268\n",
      "54/100 38.77s Tloss=0.6912 Taccu=78.56% Vloss=0.6301 Vaccu=80.25% Eta=1.65668420722\n",
      "55/100 39.47s Tloss=0.6957 Taccu=78.48% Vloss=0.634 Vaccu=80.15% Eta=1.32534736577\n",
      "56/100 40.16s Tloss=0.6948 Taccu=78.48% Vloss=0.6332 Vaccu=80.17% Eta=1.59041683893\n",
      "57/100 40.89s Tloss=0.6993 Taccu=78.44% Vloss=0.6371 Vaccu=80.11% Eta=1.27233347114\n",
      "58/100 41.62s Tloss=0.6983 Taccu=78.44% Vloss=0.6363 Vaccu=80.17% Eta=1.52680016537\n",
      "59/100 42.40s Tloss=0.7027 Taccu=78.41% Vloss=0.64 Vaccu=80.08% Eta=1.2214401323\n",
      "60/100 43.09s Tloss=0.7018 Taccu=78.42% Vloss=0.6392 Vaccu=80.08% Eta=1.46572815876\n",
      "61/100 43.83s Tloss=0.706 Taccu=78.39% Vloss=0.6429 Vaccu=80.04% Eta=1.17258252701\n",
      "62/100 44.54s Tloss=0.7051 Taccu=78.41% Vloss=0.6421 Vaccu=80.03% Eta=1.40709903241\n",
      "63/100 45.25s Tloss=0.7091 Taccu=78.36% Vloss=0.6457 Vaccu=80.11% Eta=1.12567922593\n",
      "64/100 45.95s Tloss=0.7083 Taccu=78.38% Vloss=0.645 Vaccu=80.05% Eta=1.35081507111\n",
      "65/100 46.66s Tloss=0.7121 Taccu=78.33% Vloss=0.6484 Vaccu=80.13% Eta=1.08065205689\n",
      "66/100 47.34s Tloss=0.7114 Taccu=78.34% Vloss=0.6477 Vaccu=80.1% Eta=1.29678246827\n",
      "67/100 48.03s Tloss=0.7151 Taccu=78.3% Vloss=0.6509 Vaccu=80.11% Eta=1.03742597461\n",
      "68/100 48.75s Tloss=0.7143 Taccu=78.33% Vloss=0.6502 Vaccu=80.12% Eta=1.24491116954\n",
      "69/100 49.46s Tloss=0.7178 Taccu=78.29% Vloss=0.6534 Vaccu=80.05% Eta=0.995928935629\n",
      "70/100 50.16s Tloss=0.7171 Taccu=78.3% Vloss=0.6527 Vaccu=80.11% Eta=1.19511472275\n",
      "71/100 50.97s Tloss=0.7205 Taccu=78.29% Vloss=0.6558 Vaccu=80.04% Eta=0.956091778203\n",
      "72/100 51.68s Tloss=0.7198 Taccu=78.3% Vloss=0.6551 Vaccu=80.09% Eta=1.14731013384\n",
      "73/100 52.38s Tloss=0.723 Taccu=78.25% Vloss=0.658 Vaccu=80.06% Eta=0.917848107075\n",
      "74/100 53.08s Tloss=0.7224 Taccu=78.3% Vloss=0.6574 Vaccu=80.06% Eta=1.10141772849\n",
      "75/100 53.75s Tloss=0.7255 Taccu=78.25% Vloss=0.6602 Vaccu=80.03% Eta=0.881134182792\n",
      "76/100 54.46s Tloss=0.7248 Taccu=78.28% Vloss=0.6596 Vaccu=80.04% Eta=1.05736101935\n",
      "77/100 55.16s Tloss=0.7278 Taccu=78.25% Vloss=0.6623 Vaccu=80.02% Eta=0.845888815481\n",
      "78/100 55.88s Tloss=0.7272 Taccu=78.26% Vloss=0.6617 Vaccu=80.05% Eta=1.01506657858\n",
      "79/100 56.56s Tloss=0.73 Taccu=78.24% Vloss=0.6643 Vaccu=80.03% Eta=0.812053262861\n",
      "80/100 57.26s Tloss=0.7294 Taccu=78.24% Vloss=0.6637 Vaccu=80.01% Eta=0.974463915434\n",
      "81/100 57.98s Tloss=0.7321 Taccu=78.22% Vloss=0.6662 Vaccu=80.0% Eta=0.779571132347\n",
      "82/100 59.01s Tloss=0.7315 Taccu=78.22% Vloss=0.6656 Vaccu=80.03% Eta=0.935485358816\n",
      "83/100 59.72s Tloss=0.7341 Taccu=78.22% Vloss=0.668 Vaccu=80.0% Eta=0.748388287053\n",
      "84/100 60.48s Tloss=0.7336 Taccu=78.23% Vloss=0.6675 Vaccu=80.02% Eta=0.898065944464\n",
      "85/100 61.18s Tloss=0.7361 Taccu=78.2% Vloss=0.6697 Vaccu=79.98% Eta=0.718452755571\n",
      "86/100 61.95s Tloss=0.7355 Taccu=78.23% Vloss=0.6692 Vaccu=80.0% Eta=0.862143306685\n",
      "87/100 62.69s Tloss=0.7379 Taccu=78.2% Vloss=0.6714 Vaccu=79.98% Eta=0.689714645348\n",
      "88/100 63.48s Tloss=0.7374 Taccu=78.23% Vloss=0.6709 Vaccu=79.99% Eta=0.827657574418\n",
      "89/100 64.16s Tloss=0.7397 Taccu=78.21% Vloss=0.673 Vaccu=79.99% Eta=0.662126059534\n",
      "90/100 65.09s Tloss=0.7392 Taccu=78.22% Vloss=0.6725 Vaccu=79.99% Eta=0.794551271441\n",
      "91/100 66.03s Tloss=0.7414 Taccu=78.21% Vloss=0.6745 Vaccu=79.97% Eta=0.635641017153\n",
      "92/100 66.85s Tloss=0.7409 Taccu=78.22% Vloss=0.6741 Vaccu=79.96% Eta=0.762769220583\n",
      "93/100 67.64s Tloss=0.743 Taccu=78.19% Vloss=0.6759 Vaccu=79.97% Eta=0.610215376467\n",
      "94/100 68.49s Tloss=0.7425 Taccu=78.23% Vloss=0.6755 Vaccu=79.95% Eta=0.73225845176\n",
      "95/100 69.30s Tloss=0.7445 Taccu=78.19% Vloss=0.6773 Vaccu=79.93% Eta=0.585806761408\n",
      "96/100 70.05s Tloss=0.7441 Taccu=78.22% Vloss=0.6769 Vaccu=79.92% Eta=0.70296811369\n",
      "97/100 70.84s Tloss=0.746 Taccu=78.19% Vloss=0.6787 Vaccu=79.94% Eta=0.562374490952\n",
      "98/100 71.60s Tloss=0.7455 Taccu=78.2% Vloss=0.6783 Vaccu=79.91% Eta=0.674849389142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/100 72.43s Tloss=0.7474 Taccu=78.18% Vloss=0.6799 Vaccu=79.91% Eta=0.539879511314\n",
      "100/100 73.25s Tloss=0.747 Taccu=78.19% Vloss=0.6796 Vaccu=79.9% Eta=0.647855413576\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# Data structures for plotting\n",
    "g_i = []\n",
    "g_train_loss=[]\n",
    "g_train_acc=[]\n",
    "g_valid_loss=[]\n",
    "g_valid_acc=[]\n",
    "\n",
    "#############################\n",
    "### Auxiliary variables\n",
    "#############################\n",
    "cumul_time = 0.\n",
    "n_batch = int(math.ceil(float(n_training)/batch_size))\n",
    "regularizer = None\n",
    "lamda = 0.\n",
    "last_loss=np.inf\n",
    "\n",
    "# Convert the labels to one-hot vector\n",
    "one_hot = np.zeros((n_label,n_training))\n",
    "one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "\n",
    "print('Architecture = {} | Batch size = {} | Eta = {} | Act Func={}'.format(nn_arch, batch_size, eta, acti_fun))\n",
    "\n",
    "#############################\n",
    "### Learning process\n",
    "#############################\n",
    "for i in range(n_epoch):\n",
    "    W_copy=copy.deepcopy(W)\n",
    "    B_copy=copy.deepcopy(B)\n",
    "    for j in range(n_batch):\n",
    "\n",
    "        ### Mini-batch creation\n",
    "        batch, one_hot_batch, mini_batch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "\n",
    "        prev_time = time.clock()\n",
    "\n",
    "        ### Forward propagation\n",
    "        Y, Yp = forward(act_func, W, B, batch)\n",
    "\n",
    "        ### Compute the softmax\n",
    "        out = softmax(Y[-1])\n",
    "        \n",
    "        ### Compute the gradient at the top layer\n",
    "        derror = out - one_hot_batch\n",
    "\n",
    "        ### Backpropagation\n",
    "        gradB = backward(derror, W, Yp)\n",
    "\n",
    "        ### Update the parameters\n",
    "        W, B = update(eta, batch_size, W, B, gradB, Y, regularizer, lamda)\n",
    "\n",
    "        exit()\n",
    "        curr_time = time.clock()\n",
    "        cumul_time += curr_time - prev_time\n",
    "\n",
    "    ### Training accuracy\n",
    "    train_loss, train_accuracy = computeLoss(act_func, W, B, train_set[0], train_set[1]) \n",
    "\n",
    "    ### Valid accuracy\n",
    "    valid_loss, valid_accuracy = computeLoss(act_func, W, B, valid_set[0], valid_set[1])\n",
    "    \n",
    "    if (train_loss<last_loss):\n",
    "        eta*=1.2\n",
    "    else:\n",
    "        eta*=0.8\n",
    "        W=W_copy\n",
    "        B=B_copy\n",
    "    last_loss=train_loss\n",
    "\n",
    "    g_i = np.append(g_i, i)\n",
    "    g_train_loss = np.append(g_train_loss, train_loss)\n",
    "    g_train_acc = np.append(g_train_acc, train_accuracy)\n",
    "    g_valid_loss = np.append(g_valid_loss, valid_loss)\n",
    "    g_valid_acc = np.append(g_valid_acc, valid_accuracy)\n",
    "    print('{:02d}/{} {:.2f}s Tloss={:.4} Taccu={:.4}% Vloss={:.4} Vaccu={:.4}% Eta={}'.format(i+1, n_epoch, cumul_time, train_loss, train_accuracy*100, valid_loss, valid_accuracy*100, eta))\n",
    "    sys.stdout.flush() # Force emptying the stdout buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(g_i,g_train_loss,label='train_loss')\n",
    "plt.plot(g_i,g_valid_loss,label='valid_loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Negative log-likelihood\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(g_i,1.0-g_train_acc,label='train_acc')\n",
    "plt.plot(g_i,1.0-g_valid_acc,label='valid_acc')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Classification error\")\n",
    "plt.ylim([0.,1.])\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
