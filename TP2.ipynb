{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP2:\n",
    "\n",
    "Au cours du TP1, nous avons étudié le modèle *Softmax* pour traiter le problème de classification probabiliste. Le but était de présenter deux étapes importantes de l'entraînement : la forward propagation et la mise à jour des paramètres. Le TP2 reprend le modèle Softmax dans un cadre plus général, celui des réseaux de neurones avec couches cachèes.\n",
    "Dans ce cadre, on peut considérer le modèle Softmax comme un \"module\" qui prend en entrèe des \"features\", e.g. les pixels d'une image, et qui donne en sortie une loi de probabilité sur les étiquettes.\n",
    "Un réseau de neurones est composé de plusieurs modules, transformant simplement les features d'un espace à un autre en fonction des valeurs courantes des paramètres. Ainsi, le but de l'entraînement est d'apprendre les transformations pertinentes, i.e., en modifiant les paramètres, qui permettront de réaliser la tâche associée au module de sortie. En augmentant le nombre de modules (mais aussi de fonctions non-linéaires), on augmente ainsi la complexité du modèle.\n",
    "\n",
    "Le premier but du TP2 est de programmer les trois étapes essentielles à l'entraînement d'un réseau de neurones : la *forward propagation*, la *backpropagation* et la *mise à jour des paramètres*. Vérifiez que votre modèle fonctionne. Ensuite, vous pourrez comparer les performances de votre réseau de neurones avec celles de votre modèle Softmax de la semaine dernière.\n",
    "\n",
    "Une fois ces bases réalisées, on va pouvoir ajouter plusieurs fonctions d'activations en plus de la classique *sigmoïde*: les *tanh* et *relu*. Vous pourrez comparer la sigmoïde et la tanh avec la relu, notamment lorsque l'on utilise 2 couches cachées ou plus. Vous pourrez aussi mettre en évidence le phénomène de sur-apprentissage (travaillez avec une petite sous partie des données si nécessaire).\n",
    "Pour rappel, les fonctions sont:\n",
    "\n",
    "$$ tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n",
    "\n",
    "$$ relu(x) = max(0, x) $$\n",
    "\n",
    "Remarque: La fonction relu est plus instable numériquement que les deux autres. Il est possible qu'il soit nécessaire de réduire le taux d'apprentissage (ou de l'apapter, en le réduisant au fur et à mesure que l'apprentissage progresse) ou de forcer les valeurs de la relu à rester en dessous d'une limite que l'on choisit (*clipping*).\n",
    "\n",
    "Enfin, on va implémenter la *régularisation*: on ajoutera à la fonction de mise à jour des paramètres les méthodes de régularisation *L1* et *L2*. Il s'agira ensuite de vérifier leur influence sur les courbes d'apprentissage en faisant varier le paramètre $\\lambda$.\n",
    "\n",
    "A faire: \n",
    "- Compléter les fonctions:\n",
    "    - getDimDataset\n",
    "    - sigmoid\n",
    "    - forward\n",
    "    - backward\n",
    "    - update\n",
    "    - softmax\n",
    "    - computeLoss\n",
    "    - getMiniBatch\n",
    "- Compléter les fonctions:\n",
    "    - tanh\n",
    "    - relu\n",
    "    - et faire les expériences demandées.\n",
    "- Compléter les fonctions:\n",
    "    - updateParams\n",
    "    - et faire les expériences demandées.\n",
    "- Envoyer le notebook avec le code complété avant le **13 décembre 2017** à l'adresse **labeau@limsi.fr** accompagné d'un résumé d'un maximum de 6 pages contenant des figures et des analyses rapides des expériences demandées.\n",
    "- Le résumé doit être succinct et se focaliser uniquement sur les points essentiels reliés à l'entraînement des réseaux de neurones. En plus des résultats, ce document doit décrire les difficultés que vous avez rencontrées et, dans le cas échéant, les solutions utilisées pour les résoudre. Vous pouvez aussi y décrire vos questions ouvertes et proposer une expérience sur MNIST afin d'y répondre.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 784 10\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    !wget http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "\n",
    "#####################\n",
    "# Gestion des données\n",
    "#####################  \n",
    "import dataset_loader\n",
    "train_set, valid_set, test_set = dataset_loader.load_mnist()\n",
    "\n",
    "def getDimDataset(train_set):\n",
    "    n_training = len(train_set[0])\n",
    "    n_feature = len(train_set[0][0])\n",
    "    n_label = len(set(train_set[1]))\n",
    "    return n_training, n_feature, n_label\n",
    "\n",
    "n_training, n_feature, n_label = getDimDataset(train_set)\n",
    "print(n_training,n_feature,n_label)\n",
    "\n",
    "########################\n",
    "# Gestion des paramètres\n",
    "########################\n",
    "\n",
    "# Taille de la couche cachée: sous forme de liste, il est possible\n",
    "# d'utiliser plusieurs couches cachées, avec par exemple [128, 64]\n",
    "n_hidden =[300]\n",
    "\n",
    "# Fonction d'activation: à choisir parmi 'sigmoid', 'tanh' et 'relu'\n",
    "acti_fun = 'relu'\n",
    "\n",
    "# Taille du batch\n",
    "batch_size = 500\n",
    "\n",
    "# Taux d'apprentissage:\n",
    "eta = 0.01\n",
    "\n",
    "# Nombre d'époques:\n",
    "n_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMiniBatch(i, batch_size, train_set, one_hot):\n",
    "    \"\"\"\n",
    "    Return a minibatch from the training set and the associated labels\n",
    "    Inputs: i: the identifier of the minibatch - int\n",
    "          : batch_size: the number of training examples - int\n",
    "          : train_set: the training set - ndarray\n",
    "          : one_hot: the one-hot representation of the labels - ndarray\n",
    "    Outputs: the minibatch of examples - ndarray\n",
    "           : the minibatch of labels - ndarray\n",
    "           : the number of examples in the minibatch - int\n",
    "    \"\"\"\n",
    "    idx_begin = i\n",
    "    idx_end = i+batch_size\n",
    "    batch = train_set[0][idx_begin:idx_end]\n",
    "    one_hot=one_hot.transpose()\n",
    "    one_hot_batch = one_hot[idx_begin:idx_end,:]\n",
    "    mini_batch_size = batch.shape[0]\n",
    "    return np.asfortranarray(batch), one_hot_batch, mini_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initNetwork(nn_arch, act_func_name):\n",
    "    \"\"\"\n",
    "    Initialize the neural network weights, activation function and return the number of parameters\n",
    "    Inputs: nn_arch: the number of units per hidden layer -  list of int\n",
    "          : act_func_name: the activation function name (sigmoid, tanh or relu) - str\n",
    "    Outputs: W: a list of weights for each hidden layer - list of ndarray\n",
    "           : B: a list of bias for each hidden layer - list of ndarray\n",
    "           : act_func: the activation function - function\n",
    "           : nb_params: the number of parameters  - int\n",
    "    \"\"\"\n",
    "    W,B = [],[]\n",
    "    sigma = 1.0\n",
    "    act_func = globals()[act_func_name] # Cast the string to a function\n",
    "    nb_params = 0\n",
    "\n",
    "    if act_func_name=='sigmoid':\n",
    "        sigma = 4.0\n",
    "\n",
    "    for i in range(np.size(nn_arch)-1):\n",
    "        w = np.random.normal(loc=0.0, scale=sigma/np.sqrt(nn_arch[i]), size=(nn_arch[i+1],nn_arch[i]))\n",
    "        W.append(w)\n",
    "        b = np.zeros((w.shape[0],1))\n",
    "        if act_func_name=='sigmoid':\n",
    "            b = np.sum(w,1).reshape(-1,1)/-2.0\n",
    "        B.append(b)\n",
    "        nb_params += nn_arch[i+1] * nn_arch[i] + nn_arch[i+1]\n",
    "\n",
    "    return W,B,act_func,nb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Fonctions d'activation\n",
    "########################\n",
    "\n",
    "def sigmoid(z, grad_flag=True):\n",
    "    \"\"\"\n",
    "    Perform the sigmoid transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - ndarray\n",
    "          : grad_flag: flag for computing the derivatives w.r.t. z - boolean\n",
    "    Outputs: y: the activation values - ndarray\n",
    "           : yp: the derivatives w.r.t. z - ndarray\n",
    "    \"\"\"\n",
    "    y=1.0/(1+np.exp(-1*z))\n",
    "    yp=y*(1-y)\n",
    "    return y, yp\n",
    "\n",
    "\n",
    "def tanh(z, grad_flag=True):\n",
    "    \"\"\"\n",
    "    Perform the tanh transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - ndarray\n",
    "          : grad_flag: flag for computing the derivatives w.r.t. z - boolean\n",
    "    Outputs: y: the activation values - ndarray\n",
    "    \"\"\"\n",
    "    y=(np.exp(z)-np.exp(-1*z))/(np.exp(z)+np.exp(-1*z))\n",
    "    yp=1-np.power(y,2)\n",
    "    return y, yp\n",
    "\n",
    "\n",
    "def relu(z, grad_flag=True):\n",
    "    \"\"\"\n",
    "    Perform the relu transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - ndarray\n",
    "          : grad_flag: flag for computing the derivatives w.r.t. z - boolean\n",
    "    Outputs: y: the activation values - ndarray\n",
    "    \"\"\"\n",
    "    y=np.maximum(0,z)\n",
    "    yp=np.zeros_like(y)\n",
    "    yp[y>0]=1\n",
    "    return y, yp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Création du réseau\n",
    "####################\n",
    "\n",
    "### Network Architecture\n",
    "nn_arch = np.array([n_feature] + n_hidden + [n_label])\n",
    "\n",
    "### Create the neural network\n",
    "W, B, act_func, nb_params = initNetwork(nn_arch, acti_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward(act_func, W, B, X):\n",
    "    \"\"\"\n",
    "    Perform the forward propagation\n",
    "    Inputs: act_func: the activation function - function\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias - list of ndarray\n",
    "          : X: the batch - ndarray\n",
    "    Outputs: Y: a list of activation values - list of ndarray\n",
    "           : Yp: a list of the derivatives w.r.t. the pre-activation of the activation values - list of ndarray\n",
    "    \"\"\"\n",
    "    a=X\n",
    "    Yp=[]\n",
    "    Y=[a]\n",
    "    for i in range(len(W)-1):\n",
    "        z=(a.dot(W[i].transpose()))+B[i].transpose()\n",
    "        y,yp=act_func(z)\n",
    "        Y.append(y)\n",
    "        Yp.append(yp)\n",
    "        a=y\n",
    "    \n",
    "    z=(a.dot(W[-1].transpose()))+B[-1].transpose()\n",
    "    Y.append(z)    \n",
    "    return Y, Yp\n",
    "\n",
    "#W, B, act_func, nb_params = initNetwork(nn_arch, \"sigmoid\")\n",
    "#batch=train_set[0][0:5]\n",
    "#one_hot = np.zeros((n_label,n_training))\n",
    "#one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "#batch,one_hot_batch, mini_batch_size=getMiniBatch(0, 5, train_set, one_hot)\n",
    "#Y, Yp=forward(act_func, W, B, batch)\n",
    "#print([y.shape for y in Y])\n",
    "#print([y.shape for y in Yp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "        Perform the softmax transformation to the pre-activation values\n",
    "        :param z: the pre-activation values\n",
    "        :type z: ndarray\n",
    "        :return: the activation values\n",
    "        :rtype: ndarray\n",
    "    \"\"\"\n",
    "    exps = np.exp(z-np.max(z,axis=1).reshape(-1,1))\n",
    "    somme_exps = np.sum(exps,axis=1)\n",
    "    for i in range(somme_exps.shape[0]):\n",
    "        exps[i]/=somme_exps[i]\n",
    "    return exps\n",
    "\n",
    "#out=softmax(z)\n",
    "#print(out.shape)\n",
    "#print (out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def backward(error, W, Yp): \n",
    "    \"\"\"\n",
    "    Perform the backward propagation\n",
    "    Inputs: error: the gradient w.r.t. to the last layer - ndarray\n",
    "          : W: the weights - list of ndarray\n",
    "          : Yp: the derivatives w.r.t. the pre-activation of the activation functions - list of ndarray\n",
    "    Outputs: gradb: a list of gradient w.r.t. the pre-activation with this order [gradb_layer1, ..., error] - list of ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    gradB = [error]\n",
    "    for w, yp in zip(reversed(W), reversed(Yp)):\n",
    "        error = error.dot(w) * yp\n",
    "        gradB.append(error)\n",
    "    gradB.reverse()\n",
    "    return gradB\n",
    "\n",
    "#out = softmax(Y[-1])\n",
    "### Compute the gradient at the top layer\n",
    "#derror = out - one_hot_batch\n",
    "#gradB=backward(derror, W, Yp)\n",
    "#for b in gradB: print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParams(theta, dtheta, eta, regularizer=None, lamda=0.):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: theta: the network parameters - ndarray w\n",
    "          : dtheta: the updates of the parameters - ndarray\n",
    "          : eta: the step-size of the gradient descent - float\n",
    "          : regularizer: choice of the regularizer: None, 'L1', or 'L2'\n",
    "          : lambda: hyperparamater giving the importance of the regularizer - float\n",
    "    Outputs: the parameters updated - ndarray\n",
    "    \"\"\"\n",
    "    theta=np.copy(theta)\n",
    "    if regularizer=='L1':\n",
    "        theta+=lamda*np.abs(theta)\n",
    "    elif regularizer=='L2':\n",
    "        theta+=lamda*np.power(theta,2)\n",
    "    return theta - eta * dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(eta, batch_size, W, B, gradB, Y, regularizer, lamda):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: eta: the step-size of the gradient descent - float \n",
    "          : batch_size: number of examples in the batch (for normalizing) - int\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias -  list of ndarray\n",
    "          : gradB: the gradient of the activations w.r.t. to the loss -  list of ndarray\n",
    "          : Y: the activation values -  list of ndarray\n",
    "    Outputs: W: the weights updated -  list of ndarray\n",
    "           : B: the bias updated -  list of ndarray\n",
    "    \"\"\"\n",
    "    # grad_b should be a vector: object.reshape(-1,1) can be useful\n",
    "    \n",
    "    for k in range(len(W)):\n",
    "        grad_w=((gradB[k].transpose()).dot(Y[k]))/batch_size\n",
    "        W[k]=updateParams(W[k],grad_w, eta,regularizer,lamda)\n",
    "        grad_b = (np.sum(gradB[k],0).reshape(-1,1))/batch_size\n",
    "        B[k]=updateParams(B[k],grad_b, eta,regularizer,lamda)\n",
    "        \n",
    "    return W,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLoss(act_func, W, B, X, labels):\n",
    "    \"\"\"\n",
    "    Compute the loss value of the current network on the full batch\n",
    "    Inputs: act_func: the activation function - function\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias - list of ndarray\n",
    "          : X: the batch - ndarray\n",
    "          : labels: the labels corresponding to the batch\n",
    "    Outputs: loss: the negative log-likelihood - float\n",
    "           : accuracy: the ratio of examples that are well-classified - float\n",
    "    \"\"\" \n",
    "    ### Forward propagation\n",
    "    Y, Yp = forward(act_func, W, B, X)\n",
    " \n",
    "    ### Compute the softmax and the prediction\n",
    "    out = softmax(Y[-1])\n",
    "    #pred = np.argmax(out,axis=0)\n",
    "    \n",
    "    #Loss and Accuracy    \n",
    "    loss = -np.sum(np.log(out[np.arange(out.shape[0]), labels]))/labels.shape[0]\n",
    "    accuracy = np.mean((np.argmax(out, axis=1) == labels).astype(np.float, copy=False))\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture = [784 300  10] | Batch size = 500 | Eta = 0.01 | Act Func=relu\n",
      "01/100 8.08s Tloss=1.763 Taccu=65.99% Vloss=1.751 Vaccu=67.11% Eta=0.012\n",
      "02/100 15.31s Tloss=1.24 Taccu=76.06% Vloss=1.21 Vaccu=77.78% Eta=0.0144\n",
      "03/100 22.50s Tloss=0.9074 Taccu=80.16% Vloss=0.8648 Vaccu=81.86% Eta=0.01728\n",
      "04/100 29.65s Tloss=0.7294 Taccu=82.15% Vloss=0.6797 Vaccu=84.07% Eta=0.020736\n",
      "05/100 36.75s Tloss=0.6286 Taccu=83.47% Vloss=0.5754 Vaccu=85.32% Eta=0.0248832\n",
      "06/100 43.79s Tloss=0.5679 Taccu=84.35% Vloss=0.513 Vaccu=86.29% Eta=0.02985984\n",
      "07/100 50.87s Tloss=0.531 Taccu=84.87% Vloss=0.4751 Vaccu=86.84% Eta=0.035831808\n",
      "08/100 57.89s Tloss=0.5097 Taccu=85.18% Vloss=0.4531 Vaccu=87.22% Eta=0.0429981696\n",
      "09/100 64.97s Tloss=0.4997 Taccu=85.3% Vloss=0.4421 Vaccu=87.2% Eta=0.05159780351999999\n",
      "10/100 72.06s Tloss=0.4981 Taccu=85.4% Vloss=0.4393 Vaccu=87.28% Eta=0.06191736422399999\n",
      "11/100 79.13s Tloss=0.5027 Taccu=85.34% Vloss=0.4423 Vaccu=87.32% Eta=0.05572562780159999\n",
      "12/100 86.15s Tloss=0.5018 Taccu=85.37% Vloss=0.4416 Vaccu=87.35% Eta=0.06687075336191999\n",
      "13/100 93.22s Tloss=0.5096 Taccu=85.34% Vloss=0.4477 Vaccu=87.14% Eta=0.06018367802572799\n",
      "14/100 100.29s Tloss=0.5085 Taccu=85.34% Vloss=0.4468 Vaccu=87.18% Eta=0.07222041363087359\n",
      "15/100 107.28s Tloss=0.5182 Taccu=85.29% Vloss=0.4549 Vaccu=87.08% Eta=0.06499837226778624\n",
      "16/100 114.35s Tloss=0.5169 Taccu=85.3% Vloss=0.4538 Vaccu=87.09% Eta=0.07799804672134349\n",
      "17/100 121.66s Tloss=0.5278 Taccu=85.28% Vloss=0.463 Vaccu=86.95% Eta=0.07019824204920914\n",
      "18/100 128.94s Tloss=0.5264 Taccu=85.29% Vloss=0.4617 Vaccu=86.99% Eta=0.08423789045905097\n",
      "19/100 135.95s Tloss=0.5378 Taccu=85.21% Vloss=0.4714 Vaccu=86.92% Eta=0.07581410141314587\n",
      "20/100 144.12s Tloss=0.5364 Taccu=85.23% Vloss=0.4702 Vaccu=86.96% Eta=0.09097692169577505\n",
      "21/100 151.65s Tloss=0.5476 Taccu=85.21% Vloss=0.4798 Vaccu=86.91% Eta=0.08187922952619754\n",
      "22/100 159.13s Tloss=0.5462 Taccu=85.21% Vloss=0.4785 Vaccu=86.91% Eta=0.09825507543143705\n",
      "23/100 166.46s Tloss=0.5569 Taccu=85.18% Vloss=0.4877 Vaccu=86.85% Eta=0.08842956788829334\n",
      "24/100 173.78s Tloss=0.5555 Taccu=85.19% Vloss=0.4865 Vaccu=86.83% Eta=0.10611548146595201\n",
      "25/100 180.81s Tloss=0.5657 Taccu=85.18% Vloss=0.4952 Vaccu=86.87% Eta=0.09550393331935682\n",
      "26/100 188.17s Tloss=0.5644 Taccu=85.2% Vloss=0.494 Vaccu=86.85% Eta=0.11460471998322817\n",
      "27/100 196.17s Tloss=0.5743 Taccu=85.22% Vloss=0.5025 Vaccu=86.9% Eta=0.10314424798490536\n",
      "28/100 203.85s Tloss=0.5729 Taccu=85.21% Vloss=0.5013 Vaccu=86.88% Eta=0.12377309758188643\n",
      "29/100 212.03s Tloss=0.5827 Taccu=85.26% Vloss=0.5097 Vaccu=86.94% Eta=0.11139578782369779\n",
      "30/100 219.37s Tloss=0.5813 Taccu=85.26% Vloss=0.5085 Vaccu=86.93% Eta=0.13367494538843733\n",
      "31/100 226.81s Tloss=0.5909 Taccu=85.25% Vloss=0.5168 Vaccu=86.93% Eta=0.1203074508495936\n",
      "32/100 234.25s Tloss=0.5896 Taccu=85.26% Vloss=0.5156 Vaccu=86.94% Eta=0.1443689410195123\n",
      "33/100 241.87s Tloss=0.599 Taccu=85.27% Vloss=0.5238 Vaccu=86.94% Eta=0.12993204691756108\n",
      "34/100 249.47s Tloss=0.5977 Taccu=85.27% Vloss=0.5226 Vaccu=86.96% Eta=0.1559184563010733\n",
      "35/100 257.12s Tloss=0.607 Taccu=85.28% Vloss=0.5307 Vaccu=86.95% Eta=0.14032661067096597\n",
      "36/100 264.63s Tloss=0.6057 Taccu=85.28% Vloss=0.5295 Vaccu=86.94% Eta=0.16839193280515916\n",
      "37/100 272.04s Tloss=0.6149 Taccu=85.29% Vloss=0.5375 Vaccu=86.91% Eta=0.15155273952464324\n",
      "38/100 279.47s Tloss=0.6136 Taccu=85.27% Vloss=0.5363 Vaccu=86.92% Eta=0.1818632874295719\n",
      "39/100 286.92s Tloss=0.6228 Taccu=85.3% Vloss=0.5442 Vaccu=86.89% Eta=0.1636769586866147\n",
      "40/100 294.54s Tloss=0.6215 Taccu=85.3% Vloss=0.543 Vaccu=86.9% Eta=0.19641235042393762\n",
      "41/100 302.49s Tloss=0.6306 Taccu=85.31% Vloss=0.5509 Vaccu=86.89% Eta=0.17677111538154386\n",
      "42/100 309.82s Tloss=0.6293 Taccu=85.3% Vloss=0.5497 Vaccu=86.95% Eta=0.2121253384578526\n",
      "43/100 317.18s Tloss=0.6383 Taccu=85.3% Vloss=0.5575 Vaccu=86.9% Eta=0.19091280461206736\n",
      "44/100 324.59s Tloss=0.637 Taccu=85.3% Vloss=0.5563 Vaccu=86.92% Eta=0.22909536553448082\n",
      "45/100 331.96s Tloss=0.6459 Taccu=85.31% Vloss=0.5641 Vaccu=86.93% Eta=0.20618582898103274\n",
      "46/100 340.28s Tloss=0.6447 Taccu=85.29% Vloss=0.5629 Vaccu=86.92% Eta=0.24742299477723928\n",
      "47/100 347.85s Tloss=0.6535 Taccu=85.3% Vloss=0.5706 Vaccu=86.94% Eta=0.22268069529951537\n",
      "48/100 355.57s Tloss=0.6523 Taccu=85.28% Vloss=0.5694 Vaccu=86.93% Eta=0.26721683435941845\n",
      "49/100 363.01s Tloss=0.6611 Taccu=85.31% Vloss=0.5771 Vaccu=86.97% Eta=0.24049515092347662\n",
      "50/100 370.41s Tloss=0.6599 Taccu=85.3% Vloss=0.5759 Vaccu=86.96% Eta=0.2885941811081719\n",
      "51/100 378.21s Tloss=0.6686 Taccu=85.33% Vloss=0.5835 Vaccu=86.9% Eta=0.25973476299735476\n",
      "52/100 385.64s Tloss=0.6674 Taccu=85.32% Vloss=0.5824 Vaccu=86.93% Eta=0.3116817155968257\n",
      "53/100 392.98s Tloss=0.6761 Taccu=85.34% Vloss=0.59 Vaccu=86.93% Eta=0.2805135440371432\n",
      "54/100 400.27s Tloss=0.6749 Taccu=85.34% Vloss=0.5888 Vaccu=86.92% Eta=0.3366162528445718\n",
      "55/100 407.53s Tloss=0.6836 Taccu=85.35% Vloss=0.5964 Vaccu=86.94% Eta=0.30295462756011465\n",
      "56/100 415.26s Tloss=0.6824 Taccu=85.35% Vloss=0.5952 Vaccu=86.95% Eta=0.3635455530721376\n",
      "57/100 422.37s Tloss=0.691 Taccu=85.34% Vloss=0.6027 Vaccu=86.95% Eta=0.32719099776492383\n",
      "58/100 430.49s Tloss=0.6898 Taccu=85.34% Vloss=0.6016 Vaccu=86.96% Eta=0.3926291973179086\n",
      "59/100 438.39s Tloss=0.6984 Taccu=85.33% Vloss=0.6091 Vaccu=86.95% Eta=0.3533662775861177\n",
      "60/100 445.68s Tloss=0.6973 Taccu=85.33% Vloss=0.608 Vaccu=86.95% Eta=0.42403953310334125\n",
      "61/100 452.91s Tloss=0.7058 Taccu=85.34% Vloss=0.6155 Vaccu=86.96% Eta=0.3816355797930071\n",
      "62/100 460.14s Tloss=0.7047 Taccu=85.35% Vloss=0.6144 Vaccu=86.97% Eta=0.45796269575160853\n",
      "63/100 467.39s Tloss=0.7132 Taccu=85.35% Vloss=0.6218 Vaccu=86.97% Eta=0.41216642617644766\n",
      "64/100 474.70s Tloss=0.7121 Taccu=85.36% Vloss=0.6207 Vaccu=86.95% Eta=0.4945997114117372\n",
      "65/100 482.10s Tloss=0.7206 Taccu=85.36% Vloss=0.6281 Vaccu=86.93% Eta=0.44513974027056347\n",
      "66/100 489.64s Tloss=0.7194 Taccu=85.37% Vloss=0.627 Vaccu=86.93% Eta=0.5341676883246761\n",
      "67/100 496.98s Tloss=0.7279 Taccu=85.36% Vloss=0.6344 Vaccu=86.94% Eta=0.4807509194922085\n",
      "68/100 504.19s Tloss=0.7268 Taccu=85.37% Vloss=0.6333 Vaccu=86.93% Eta=0.5769011033906501\n",
      "69/100 511.48s Tloss=0.7351 Taccu=85.37% Vloss=0.6406 Vaccu=86.93% Eta=0.5192109930515851\n",
      "70/100 518.84s Tloss=0.734 Taccu=85.37% Vloss=0.6396 Vaccu=86.94% Eta=0.6230531916619021\n",
      "71/100 526.14s Tloss=0.7424 Taccu=85.36% Vloss=0.6468 Vaccu=86.94% Eta=0.5607478724957119\n",
      "72/100 533.85s Tloss=0.7413 Taccu=85.37% Vloss=0.6458 Vaccu=86.96% Eta=0.6728974469948542\n",
      "73/100 541.13s Tloss=0.7496 Taccu=85.35% Vloss=0.6531 Vaccu=86.95% Eta=0.6056077022953688\n",
      "74/100 548.29s Tloss=0.7485 Taccu=85.36% Vloss=0.652 Vaccu=86.98% Eta=0.7267292427544425\n",
      "75/100 555.63s Tloss=0.7568 Taccu=85.34% Vloss=0.6593 Vaccu=86.96% Eta=0.6540563184789983\n",
      "76/100 562.91s Tloss=0.7557 Taccu=85.35% Vloss=0.6582 Vaccu=87.02% Eta=0.7848675821747979\n",
      "77/100 570.13s Tloss=0.764 Taccu=85.35% Vloss=0.6654 Vaccu=87.03% Eta=0.7063808239573182\n",
      "78/100 578.36s Tloss=0.7629 Taccu=85.35% Vloss=0.6644 Vaccu=87.0% Eta=0.8476569887487818\n",
      "79/100 585.61s Tloss=0.7712 Taccu=85.32% Vloss=0.6716 Vaccu=87.02% Eta=0.7628912898739036\n",
      "80/100 592.98s Tloss=0.7701 Taccu=85.32% Vloss=0.6706 Vaccu=87.0% Eta=0.9154695478486843\n",
      "81/100 601.08s Tloss=0.7783 Taccu=85.31% Vloss=0.6777 Vaccu=87.02% Eta=0.8239225930638159\n",
      "82/100 608.60s Tloss=0.7772 Taccu=85.32% Vloss=0.6767 Vaccu=86.99% Eta=0.988707111676579\n",
      "83/100 616.29s Tloss=0.7854 Taccu=85.3% Vloss=0.6839 Vaccu=87.02% Eta=0.8898364005089211\n",
      "84/100 623.75s Tloss=0.7843 Taccu=85.31% Vloss=0.6828 Vaccu=86.99% Eta=1.0678036806107052\n",
      "85/100 631.07s Tloss=0.7925 Taccu=85.31% Vloss=0.69 Vaccu=87.02% Eta=0.9610233125496347\n",
      "86/100 638.52s Tloss=0.7914 Taccu=85.31% Vloss=0.689 Vaccu=87.02% Eta=1.1532279750595615\n",
      "87/100 646.51s Tloss=0.7996 Taccu=85.32% Vloss=0.6961 Vaccu=87.04% Eta=1.0379051775536055\n",
      "88/100 654.08s Tloss=0.7985 Taccu=85.32% Vloss=0.6951 Vaccu=87.01% Eta=1.2454862130643265\n",
      "89/100 661.76s Tloss=0.8066 Taccu=85.31% Vloss=0.7022 Vaccu=87.04% Eta=1.1209375917578939\n",
      "90/100 669.29s Tloss=0.8056 Taccu=85.32% Vloss=0.7012 Vaccu=87.03% Eta=1.3451251101094726\n",
      "91/100 676.96s Tloss=0.8137 Taccu=85.3% Vloss=0.7083 Vaccu=87.04% Eta=1.2106125990985253\n",
      "92/100 684.37s Tloss=0.8127 Taccu=85.31% Vloss=0.7073 Vaccu=87.01% Eta=1.4527351189182303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/100 692.18s Tloss=0.8208 Taccu=85.31% Vloss=0.7144 Vaccu=87.03% Eta=1.3074616070264073\n",
      "94/100 699.32s Tloss=0.8197 Taccu=85.31% Vloss=0.7134 Vaccu=87.03% Eta=1.5689539284316887\n",
      "95/100 706.39s Tloss=0.8278 Taccu=85.31% Vloss=0.7205 Vaccu=87.02% Eta=1.4120585355885198\n",
      "96/100 713.68s Tloss=0.8268 Taccu=85.31% Vloss=0.7195 Vaccu=87.02% Eta=1.6944702427062237\n",
      "97/100 721.06s Tloss=0.8348 Taccu=85.31% Vloss=0.7266 Vaccu=87.0% Eta=1.5250232184356014\n",
      "98/100 728.61s Tloss=0.8338 Taccu=85.31% Vloss=0.7256 Vaccu=86.99% Eta=1.8300278621227215\n",
      "99/100 735.86s Tloss=0.8419 Taccu=85.3% Vloss=0.7327 Vaccu=87.01% Eta=1.6470250759104494\n",
      "100/100 742.89s Tloss=0.8408 Taccu=85.3% Vloss=0.7317 Vaccu=87.0% Eta=1.9764300910925392\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# Data structures for plotting\n",
    "g_i = []\n",
    "g_train_loss=[]\n",
    "g_train_acc=[]\n",
    "g_valid_loss=[]\n",
    "g_valid_acc=[]\n",
    "\n",
    "#############################\n",
    "### Auxiliary variables\n",
    "#############################\n",
    "cumul_time = 0.\n",
    "n_batch = int(math.ceil(float(n_training)/batch_size))\n",
    "regularizer = None\n",
    "lamda = 0.\n",
    "last_loss=np.inf\n",
    "\n",
    "# Convert the labels to one-hot vector\n",
    "one_hot = np.zeros((n_label,n_training))\n",
    "one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "\n",
    "print('Architecture = {} | Batch size = {} | Eta = {} | Act Func={}'.format(nn_arch, batch_size, eta, acti_fun))\n",
    "\n",
    "#############################\n",
    "### Learning process\n",
    "#############################\n",
    "for i in range(n_epoch):\n",
    "    W_copy=copy.deepcopy(W)\n",
    "    B_copy=copy.deepcopy(B)\n",
    "    for j in range(n_batch):\n",
    "\n",
    "        ### Mini-batch creation\n",
    "        batch, one_hot_batch, mini_batch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "\n",
    "        prev_time = time.clock()\n",
    "\n",
    "        ### Forward propagation\n",
    "        Y, Yp = forward(act_func, W, B, batch)\n",
    "\n",
    "        ### Compute the softmax\n",
    "        out = softmax(Y[-1])\n",
    "        \n",
    "        ### Compute the gradient at the top layer\n",
    "        derror = out - one_hot_batch\n",
    "\n",
    "        ### Backpropagation\n",
    "        gradB = backward(derror, W, Yp)\n",
    "\n",
    "        ### Update the parameters\n",
    "        W, B = update(eta, batch_size, W, B, gradB, Y, regularizer, lamda)\n",
    "\n",
    "        curr_time = time.clock()\n",
    "        cumul_time += curr_time - prev_time\n",
    "\n",
    "    ### Training accuracy\n",
    "    train_loss, train_accuracy = computeLoss(act_func, W, B, train_set[0], train_set[1]) \n",
    "\n",
    "    ### Valid accuracy\n",
    "    valid_loss, valid_accuracy = computeLoss(act_func, W, B, valid_set[0], valid_set[1])\n",
    "    \n",
    "    if (train_loss<last_loss):\n",
    "        eta*=1.2\n",
    "    else:\n",
    "        eta*=0.9\n",
    "        W=W_copy\n",
    "        B=B_copy\n",
    "    last_loss=train_loss\n",
    "\n",
    "    g_i = np.append(g_i, i)\n",
    "    g_train_loss = np.append(g_train_loss, train_loss)\n",
    "    g_train_acc = np.append(g_train_acc, train_accuracy)\n",
    "    g_valid_loss = np.append(g_valid_loss, valid_loss)\n",
    "    g_valid_acc = np.append(g_valid_acc, valid_accuracy)\n",
    "    print('{:02d}/{} {:.2f}s Tloss={:.4} Taccu={:.4}% Vloss={:.4} Vaccu={:.4}% Eta={}'.format(i+1, n_epoch, cumul_time, train_loss, train_accuracy*100, valid_loss, valid_accuracy*100, eta))\n",
    "    sys.stdout.flush() # Force emptying the stdout buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f8088a7f2b0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEPCAYAAAC3NDh4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngUNFGQpeBm4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8088a99860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_i,g_train_loss,label='train_loss')\n",
    "plt.plot(g_i,g_valid_loss,label='valid_loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Negative log-likelihood\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f80889b92b0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEPCAYAAAC3NDh4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC+WS0usDBOsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8088a19198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_i,g_train_acc,label='train_acc')\n",
    "plt.plot(g_i,g_valid_acc,label='valid_acc')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([0.,1.])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
