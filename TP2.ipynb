{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP2:\n",
    "\n",
    "Au cours du TP1, nous avons étudié le modèle *Softmax* pour traiter le problème de classification probabiliste. Le but était de présenter deux étapes importantes de l'entraînement : la forward propagation et la mise à jour des paramètres. Le TP2 reprend le modèle Softmax dans un cadre plus général, celui des réseaux de neurones avec couches cachèes.\n",
    "Dans ce cadre, on peut considérer le modèle Softmax comme un \"module\" qui prend en entrèe des \"features\", e.g. les pixels d'une image, et qui donne en sortie une loi de probabilité sur les étiquettes.\n",
    "Un réseau de neurones est composé de plusieurs modules, transformant simplement les features d'un espace à un autre en fonction des valeurs courantes des paramètres. Ainsi, le but de l'entraînement est d'apprendre les transformations pertinentes, i.e., en modifiant les paramètres, qui permettront de réaliser la tâche associée au module de sortie. En augmentant le nombre de modules (mais aussi de fonctions non-linéaires), on augmente ainsi la complexité du modèle.\n",
    "\n",
    "Le premier but du TP2 est de programmer les trois étapes essentielles à l'entraînement d'un réseau de neurones : la *forward propagation*, la *backpropagation* et la *mise à jour des paramètres*. Vérifiez que votre modèle fonctionne. Ensuite, vous pourrez comparer les performances de votre réseau de neurones avec celles de votre modèle Softmax de la semaine dernière.\n",
    "\n",
    "Une fois ces bases réalisées, on va pouvoir ajouter plusieurs fonctions d'activations en plus de la classique *sigmoïde*: les *tanh* et *relu*. Vous pourrez comparer la sigmoïde et la tanh avec la relu, notamment lorsque l'on utilise 2 couches cachées ou plus. Vous pourrez aussi mettre en évidence le phénomène de sur-apprentissage (travaillez avec une petite sous partie des données si nécessaire).\n",
    "Pour rappel, les fonctions sont:\n",
    "\n",
    "$$ tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n",
    "\n",
    "$$ relu(x) = max(0, x) $$\n",
    "\n",
    "Remarque: La fonction relu est plus instable numériquement que les deux autres. Il est possible qu'il soit nécessaire de réduire le taux d'apprentissage (ou de l'apapter, en le réduisant au fur et à mesure que l'apprentissage progresse) ou de forcer les valeurs de la relu à rester en dessous d'une limite que l'on choisit (*clipping*).\n",
    "\n",
    "Enfin, on va implémenter la *régularisation*: on ajoutera à la fonction de mise à jour des paramètres les méthodes de régularisation *L1* et *L2*. Il s'agira ensuite de vérifier leur influence sur les courbes d'apprentissage en faisant varier le paramètre $\\lambda$.\n",
    "\n",
    "A faire: \n",
    "- Compléter les fonctions:\n",
    "    - getDimDataset\n",
    "    - sigmoid\n",
    "    - forward\n",
    "    - backward\n",
    "    - update\n",
    "    - softmax\n",
    "    - computeLoss\n",
    "    - getMiniBatch\n",
    "- Compléter les fonctions:\n",
    "    - tanh\n",
    "    - relu\n",
    "    - et faire les expériences demandées.\n",
    "- Compléter les fonctions:\n",
    "    - updateParams\n",
    "    - et faire les expériences demandées.\n",
    "- Envoyer le notebook avec le code complété avant le **13 décembre 2017** à l'adresse **labeau@limsi.fr** accompagné d'un résumé d'un maximum de 6 pages contenant des figures et des analyses rapides des expériences demandées.\n",
    "- Le résumé doit être succinct et se focaliser uniquement sur les points essentiels reliés à l'entraînement des réseaux de neurones. En plus des résultats, ce document doit décrire les difficultés que vous avez rencontrées et, dans le cas échéant, les solutions utilisées pour les résoudre. Vous pouvez aussi y décrire vos questions ouvertes et proposer une expérience sur MNIST afin d'y répondre.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 784 10\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    !wget http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "\n",
    "#####################\n",
    "# Gestion des données\n",
    "#####################  \n",
    "import dataset_loader\n",
    "train_set, valid_set, test_set = dataset_loader.load_mnist()\n",
    "\n",
    "def getDimDataset(train_set):\n",
    "    n_training = len(train_set[0])\n",
    "    n_feature = len(train_set[0][0])\n",
    "    n_label = len(set(train_set[1]))\n",
    "    return n_training, n_feature, n_label\n",
    "\n",
    "n_training, n_feature, n_label = getDimDataset(train_set)\n",
    "print(n_training,n_feature,n_label)\n",
    "########################\n",
    "# Gestion des paramètres\n",
    "########################\n",
    "\n",
    "# Taille de la couche cachée: sous forme de liste, il est possible\n",
    "# d'utiliser plusieurs couches cachées, avec par exemple [128, 64]\n",
    "n_hidden =[256]\n",
    "\n",
    "# Fonction d'activation: à choisir parmi 'sigmoid', 'tanh' et 'relu'\n",
    "act_func = 'tanh'\n",
    "\n",
    "# Taille du batch\n",
    "batch_size = 500\n",
    "\n",
    "# Taux d'apprentissage:\n",
    "eta = 0.001\n",
    "\n",
    "# Nombre d'époques:\n",
    "n_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMiniBatch(i, batch_size, train_set, one_hot):\n",
    "    \"\"\"\n",
    "    Return a minibatch from the training set and the associated labels\n",
    "    Inputs: i: the identifier of the minibatch - int\n",
    "          : batch_size: the number of training examples - int\n",
    "          : train_set: the training set - ndarray\n",
    "          : one_hot: the one-hot representation of the labels - ndarray\n",
    "    Outputs: the minibatch of examples - ndarray\n",
    "           : the minibatch of labels - ndarray\n",
    "           : the number of examples in the minibatch - int\n",
    "    \"\"\"\n",
    "    #one_hot_batch = one_hot[:,idx_begin:idx_end]\n",
    "    idx_begin = i\n",
    "    idx_end = i+batch_size\n",
    "    batch = train_set[0][idx_begin:idx_end]\n",
    "    one_hot=one_hot.transpose()\n",
    "    one_hot_batch = one_hot[idx_begin:idx_end,:]\n",
    "    mini_batch_size = batch.shape[0]\n",
    "    return np.asfortranarray(batch), one_hot_batch, mini_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initNetwork(nn_arch, act_func_name):\n",
    "    \"\"\"\n",
    "    Initialize the neural network weights, activation function and return the number of parameters\n",
    "    Inputs: nn_arch: the number of units per hidden layer -  list of int\n",
    "          : act_func_name: the activation function name (sigmoid, tanh or relu) - str\n",
    "    Outputs: W: a list of weights for each hidden layer - list of ndarray\n",
    "           : B: a list of bias for each hidden layer - list of ndarray\n",
    "           : act_func: the activation function - function\n",
    "           : nb_params: the number of parameters  - int\n",
    "    \"\"\"\n",
    "\n",
    "    W,B = [],[]\n",
    "    sigma = 1.0\n",
    "    act_func = globals()[act_func_name] # Cast the string to a function\n",
    "    nb_params = 0\n",
    "\n",
    "    if act_func_name=='sigmoid':\n",
    "        sigma = 4.0\n",
    "\n",
    "    for i in range(np.size(nn_arch)-1):\n",
    "        w = np.random.normal(loc=0.0, scale=sigma/np.sqrt(nn_arch[i]), size=(nn_arch[i+1],nn_arch[i]))\n",
    "        W.append(w)\n",
    "        b = np.zeros((w.shape[0],1))\n",
    "        if act_func_name=='sigmoid':\n",
    "            b = np.sum(w,1).reshape(-1,1)/-2.0\n",
    "        B.append(b)\n",
    "        nb_params += nn_arch[i+1] * nn_arch[i] + nn_arch[i+1]\n",
    "\n",
    "    return W,B,act_func,nb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Fonctions d'activation\n",
    "########################\n",
    "\n",
    "def sigmoid(z, grad_flag=True):\n",
    "    \"\"\"\n",
    "    Perform the sigmoid transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - ndarray\n",
    "          : grad_flag: flag for computing the derivatives w.r.t. z - boolean\n",
    "    Outputs: y: the activation values - ndarray\n",
    "           : yp: the derivatives w.r.t. z - ndarray\n",
    "    \"\"\"\n",
    "    y=1.0/(1+np.exp(-1*z))\n",
    "    yp=y*(1-y)\n",
    "    return y, yp\n",
    "\n",
    "# A compléter une fois que la première partie du TP finie:\n",
    "\n",
    "def tanh(z, grad_flag=True):\n",
    "    \"\"\"\n",
    "    Perform the tanh transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - ndarray\n",
    "          : grad_flag: flag for computing the derivatives w.r.t. z - boolean\n",
    "    Outputs: y: the activation values - ndarray\n",
    "    \"\"\"\n",
    "    y=(np.exp(z)-np.exp(-1*z))/(np.exp(z)+np.exp(-1*z))\n",
    "    yp=1-np.power(y,2)\n",
    "    return y, yp\n",
    "\n",
    "\n",
    "def relu(z, grad_flag=True):\n",
    "    \"\"\"\n",
    "    Perform the relu transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - ndarray\n",
    "          : grad_flag: flag for computing the derivatives w.r.t. z - boolean\n",
    "    Outputs: y: the activation values - ndarray\n",
    "    \"\"\"\n",
    "    y=np.maximum(0,z)\n",
    "    yp=np.zeros_like(y)\n",
    "    yp[y>0]=1\n",
    "    return y, yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 784)\n",
      "(10, 256)\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Création du réseau\n",
    "####################\n",
    "\n",
    "### Network Architecture\n",
    "nn_arch = np.array([n_feature] + n_hidden + [n_label])\n",
    "\n",
    "### Create the neural network\n",
    "W, B, act_func, nb_params = initNetwork(nn_arch, act_func)\n",
    "for w in W:\n",
    "        print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 784), (5, 256), (5, 10)]\n",
      "[(5, 256)]\n"
     ]
    }
   ],
   "source": [
    "def forward(act_func, W, B, X):\n",
    "    \"\"\"\n",
    "    Perform the forward propagation\n",
    "    Inputs: act_func: the activation function - function\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias - list of ndarray\n",
    "          : X: the batch - ndarray\n",
    "    Outputs: Y: a list of activation values - list of ndarray\n",
    "           : Yp: a list of the derivatives w.r.t. the pre-activation of the activation values - list of ndarray\n",
    "    \"\"\"\n",
    "    a=X\n",
    "    Yp=[]\n",
    "    Y=[a]\n",
    "    for i in range(len(W)-1):\n",
    "        z=(a.dot(W[i].transpose()))+B[i].transpose()\n",
    "        y,yp=act_func(z)\n",
    "        Y.append(y)\n",
    "        Yp.append(yp)\n",
    "        a=y\n",
    "    \n",
    "    z=(a.dot(W[-1].transpose()))+B[-1].transpose()\n",
    "    Y.append(z)    \n",
    "    return Y, Yp\n",
    "W, B, act_func, nb_params = initNetwork(nn_arch, \"sigmoid\")\n",
    "#batch=train_set[0][0:5]\n",
    "one_hot = np.zeros((n_label,n_training))\n",
    "one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "batch,one_hot_batch, mini_batch_size=getMiniBatch(0, 5, train_set, one_hot)\n",
    "Y, Yp=forward(act_func, W, B, batch)\n",
    "print([y.shape for y in Y])\n",
    "print([y.shape for y in Yp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "        Perform the softmax transformation to the pre-activation values\n",
    "        :param z: the pre-activation values\n",
    "        :type z: ndarray\n",
    "        :return: the activation values\n",
    "        :rtype: ndarray\n",
    "    \"\"\"\n",
    "    exps = np.exp(z-np.max(z,axis=1).reshape(-1,1))\n",
    "    somme_exps = np.sum(exps,axis=1)\n",
    "    for i in range(somme_exps.shape[0]):\n",
    "        exps[i]/=somme_exps[i]\n",
    "    return exps\n",
    "#out=softmax(z)\n",
    "#print(out.shape)\n",
    "#print (out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 256)\n",
      "(5, 10)\n"
     ]
    }
   ],
   "source": [
    "def backward(error, W, Yp): \n",
    "    \"\"\"\n",
    "    Perform the backward propagation\n",
    "    Inputs: error: the gradient w.r.t. to the last layer - ndarray\n",
    "          : W: the weights - list of ndarray\n",
    "          : Yp: the derivatives w.r.t. the pre-activation of the activation functions - list of ndarray\n",
    "    Outputs: gradb: a list of gradient w.r.t. the pre-activation with this order [gradb_layer1, ..., error] - list of ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    gradB = [error]\n",
    "    for w, yp in zip(reversed(W), reversed(Yp)):\n",
    "        error = error.dot(w) * yp\n",
    "        gradB.append(error)\n",
    "    gradB.reverse()\n",
    "    return gradB\n",
    "\n",
    "out = softmax(Y[-1])\n",
    "### Compute the gradient at the top layer\n",
    "derror = out - one_hot_batch\n",
    "gradB=backward(derror, W, Yp)\n",
    "for b in gradB: print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParams(theta, dtheta, eta, regularizer=None, lamda=0.):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: theta: the network parameters - ndarray w\n",
    "          : dtheta: the updates of the parameters - ndarray\n",
    "          : eta: the step-size of the gradient descent - float\n",
    "          : regularizer: choice of the regularizer: None, 'L1', or 'L2'\n",
    "          : lambda: hyperparamater giving the importance of the regularizer - float\n",
    "    Outputs: the parameters updated - ndarray\n",
    "    \"\"\"\n",
    "    theta=np.copy(theta)\n",
    "    if regularizer=='L1':\n",
    "        theta+=lamda*np.abs(theta)\n",
    "    elif regularizer=='L2':\n",
    "        theta+=lamda*np.power(theta,2)\n",
    "    return theta - eta * dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(eta, batch_size, W, B, gradB, Y, regularizer, lamda):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: eta: the step-size of the gradient descent - float \n",
    "          : batch_size: number of examples in the batch (for normalizing) - int\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias -  list of ndarray\n",
    "          : gradB: the gradient of the activations w.r.t. to the loss -  list of ndarray\n",
    "          : Y: the activation values -  list of ndarray\n",
    "    Outputs: W: the weights updated -  list of ndarray\n",
    "           : B: the bias updated -  list of ndarray\n",
    "    \"\"\"\n",
    "    # grad_b should be a vector: object.reshape(-1,1) can be useful\n",
    "    \n",
    "    accumulated_gradients_W = [np.zeros_like(w) for w in W]\n",
    "    for i in range(len(W)):\n",
    "        for x in range(batch_size):\n",
    "            accumulated_gradients_W[i] += (gradB[i][x].reshape(-1,1)).transpose().dot(Y[i+1][x])\n",
    "        W[i]=updateParams(W[i], accumulated_gradients_W[i]/batch_size, eta,regularizer,lamda)\n",
    "        B[i]=updateParams(B[i], np.sum(gradB[i], axis=0, keepdims=True).transpose()/batch_size, eta,regularizer,lamda)\n",
    "\n",
    "    return W, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLoss(act_func, W, B, X, labels):\n",
    "    \"\"\"\n",
    "    Compute the loss value of the current network on the full batch\n",
    "    Inputs: act_func: the activation function - function\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias - list of ndarray\n",
    "          : X: the batch - ndarray\n",
    "          : labels: the labels corresponding to the batch\n",
    "    Outputs: loss: the negative log-likelihood - float\n",
    "           : accuracy: the ratio of examples that are well-classified - float\n",
    "    \"\"\" \n",
    "    ### Forward propagation\n",
    "    Y, Yp = forward(act_func, W, B, X)\n",
    " \n",
    "    ### Compute the softmax and the prediction\n",
    "    out = softmax(Y[-1])\n",
    "    #pred = np.argmax(out,axis=0)\n",
    "    \n",
    "    #Loss and Accuracy    \n",
    "    loss = -np.sum(np.log(out[np.arange(out.shape[0]), labels]))/labels.shape[0]\n",
    "    accuracy = np.mean((np.argmax(out, axis=1) == labels).astype(np.float, copy=False))\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture = [784 256  10] | Batch size = 500 | Eta = 0.001\n",
      "01/50 26.63s Tloss=3.195 Taccu=0.1217 Vloss=3.206 Vaccu=0.1157 Eta=0.0012\n",
      "02/50 53.48s Tloss=3.058 Taccu=0.1292 Vloss=3.084 Vaccu=0.1254 Eta=0.0014399999999999999\n",
      "03/50 80.41s Tloss=2.866 Taccu=0.1291 Vloss=2.889 Vaccu=0.1315 Eta=0.0017279999999999997\n",
      "04/50 107.00s Tloss=2.805 Taccu=0.1264 Vloss=2.824 Vaccu=0.1314 Eta=0.0020735999999999997\n",
      "05/50 133.71s Tloss=2.776 Taccu=0.1245 Vloss=2.792 Vaccu=0.1316 Eta=0.0024883199999999996\n",
      "06/50 160.44s Tloss=2.755 Taccu=0.1231 Vloss=2.771 Vaccu=0.1323 Eta=0.0029859839999999993\n",
      "07/50 187.26s Tloss=2.738 Taccu=0.1223 Vloss=2.752 Vaccu=0.1323 Eta=0.003583180799999999\n",
      "08/50 214.02s Tloss=2.72 Taccu=0.1221 Vloss=2.734 Vaccu=0.1307 Eta=0.0042998169599999985\n",
      "09/50 240.84s Tloss=2.701 Taccu=0.1215 Vloss=2.715 Vaccu=0.1312 Eta=0.005159780351999998\n",
      "10/50 267.49s Tloss=2.682 Taccu=0.1297 Vloss=2.695 Vaccu=0.1401 Eta=0.0061917364223999976\n",
      "11/50 294.19s Tloss=2.66 Taccu=0.1371 Vloss=2.673 Vaccu=0.146 Eta=0.007430083706879997\n",
      "12/50 320.89s Tloss=2.637 Taccu=0.1439 Vloss=2.65 Vaccu=0.1499 Eta=0.008916100448255996\n",
      "13/50 347.49s Tloss=2.612 Taccu=0.1488 Vloss=2.625 Vaccu=0.1537 Eta=0.010699320537907194\n",
      "14/50 374.27s Tloss=2.586 Taccu=0.1538 Vloss=2.598 Vaccu=0.1582 Eta=0.012839184645488633\n",
      "15/50 401.04s Tloss=2.558 Taccu=0.1568 Vloss=2.57 Vaccu=0.1597 Eta=0.01540702157458636\n",
      "16/50 427.81s Tloss=2.529 Taccu=0.1585 Vloss=2.541 Vaccu=0.1617 Eta=0.01848842588950363\n",
      "17/50 454.55s Tloss=2.5 Taccu=0.1594 Vloss=2.51 Vaccu=0.1626 Eta=0.022186111067404354\n",
      "18/50 481.32s Tloss=2.47 Taccu=0.159 Vloss=2.48 Vaccu=0.1608 Eta=0.026623333280885224\n",
      "19/50 508.07s Tloss=2.44 Taccu=0.1573 Vloss=2.448 Vaccu=0.1575 Eta=0.031947999937062266\n",
      "20/50 534.74s Tloss=2.41 Taccu=0.1542 Vloss=2.417 Vaccu=0.1525 Eta=0.03833759992447472\n",
      "21/50 562.91s Tloss=2.38 Taccu=0.1491 Vloss=2.385 Vaccu=0.1471 Eta=0.04600511990936966\n",
      "22/50 591.74s Tloss=2.35 Taccu=0.1433 Vloss=2.354 Vaccu=0.1465 Eta=0.05520614389124359\n",
      "23/50 620.33s Tloss=2.321 Taccu=0.1525 Vloss=2.323 Vaccu=0.154 Eta=0.06624737266949231\n",
      "24/50 647.32s Tloss=2.296 Taccu=0.1565 Vloss=2.296 Vaccu=0.1595 Eta=0.07949684720339077\n",
      "25/50 675.96s Tloss=2.277 Taccu=0.1614 Vloss=2.276 Vaccu=0.1646 Eta=0.09539621664406893\n",
      "26/50 703.93s Tloss=2.264 Taccu=0.1644 Vloss=2.262 Vaccu=0.1683 Eta=0.1144754599728827\n",
      "27/50 731.36s Tloss=2.255 Taccu=0.167 Vloss=2.252 Vaccu=0.1703 Eta=0.13737055196745923\n",
      "28/50 758.52s Tloss=2.247 Taccu=0.1698 Vloss=2.243 Vaccu=0.172 Eta=0.16484466236095108\n",
      "29/50 785.36s Tloss=2.238 Taccu=0.1711 Vloss=2.234 Vaccu=0.1732 Eta=0.1978135948331413\n",
      "30/50 812.95s Tloss=2.229 Taccu=0.1724 Vloss=2.225 Vaccu=0.1739 Eta=0.23737631379976953\n",
      "31/50 840.30s Tloss=2.22 Taccu=0.1735 Vloss=2.215 Vaccu=0.1756 Eta=0.28485157655972343\n",
      "32/50 867.02s Tloss=2.209 Taccu=0.1747 Vloss=2.204 Vaccu=0.1771 Eta=0.3418218918716681\n",
      "33/50 893.93s Tloss=2.199 Taccu=0.1774 Vloss=2.192 Vaccu=0.1796 Eta=0.41018627024600174\n",
      "34/50 920.62s Tloss=2.187 Taccu=0.18 Vloss=2.18 Vaccu=0.1824 Eta=0.49222352429520205\n",
      "35/50 947.51s Tloss=2.2 Taccu=0.1806 Vloss=2.197 Vaccu=0.1804 Eta=0.39377881943616166\n",
      "36/50 973.95s Tloss=2.177 Taccu=0.1831 Vloss=2.169 Vaccu=0.1868 Eta=0.472534583323394\n",
      "37/50 1000.86s Tloss=2.193 Taccu=0.1839 Vloss=2.189 Vaccu=0.1838 Eta=0.37802766665871523\n",
      "38/50 1027.70s Tloss=2.164 Taccu=0.1867 Vloss=2.154 Vaccu=0.1923 Eta=0.45363319999045826\n",
      "39/50 1054.46s Tloss=2.161 Taccu=0.189 Vloss=2.147 Vaccu=0.1971 Eta=0.5443598399885499\n",
      "40/50 1081.24s Tloss=2.231 Taccu=0.1602 Vloss=2.215 Vaccu=0.1649 Eta=0.43548787199083994\n",
      "41/50 1108.15s Tloss=2.155 Taccu=0.192 Vloss=2.141 Vaccu=0.2 Eta=0.522585446389008\n",
      "42/50 1135.37s Tloss=2.191 Taccu=0.1924 Vloss=2.19 Vaccu=0.1895 Eta=0.4180683571112064\n",
      "43/50 1162.50s Tloss=2.15 Taccu=0.1943 Vloss=2.136 Vaccu=0.2029 Eta=0.5016820285334477\n",
      "44/50 1189.38s Tloss=2.164 Taccu=0.1908 Vloss=2.148 Vaccu=0.1961 Eta=0.4013456228267582\n",
      "45/50 1216.31s Tloss=2.146 Taccu=0.1959 Vloss=2.131 Vaccu=0.207 Eta=0.4816147473921098\n",
      "46/50 1243.43s Tloss=2.138 Taccu=0.2069 Vloss=2.131 Vaccu=0.208 Eta=0.5779376968705318\n",
      "47/50 1271.53s Tloss=2.195 Taccu=0.179 Vloss=2.179 Vaccu=0.1844 Eta=0.46235015749642544\n",
      "48/50 1298.20s Tloss=2.12 Taccu=0.2112 Vloss=2.11 Vaccu=0.2168 Eta=0.5548201889957105\n",
      "49/50 1325.03s Tloss=2.109 Taccu=0.2198 Vloss=2.097 Vaccu=0.2293 Eta=0.6657842267948526\n",
      "50/50 1351.75s Tloss=2.191 Taccu=0.2058 Vloss=2.193 Vaccu=0.2 Eta=0.5326273814358821\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# Data structures for plotting\n",
    "g_i = []\n",
    "g_train_loss=[]\n",
    "g_train_acc=[]\n",
    "g_valid_loss=[]\n",
    "g_valid_acc=[]\n",
    "\n",
    "#############################\n",
    "### Auxiliary variables\n",
    "#############################\n",
    "cumul_time = 0.\n",
    "n_batch = int(math.ceil(float(n_training)/batch_size))\n",
    "regularizer = None\n",
    "lamda = 0.\n",
    "last_loss=np.inf\n",
    "\n",
    "# Convert the labels to one-hot vector\n",
    "one_hot = np.zeros((n_label,n_training))\n",
    "one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "\n",
    "print('Architecture = {} | Batch size = {} | Eta = {}'.format(nn_arch, batch_size, eta))\n",
    "\n",
    "#############################\n",
    "### Learning process\n",
    "#############################\n",
    "for i in range(n_epoch):\n",
    "    W_copy=copy.deepcopy(W)\n",
    "    B_copy=copy.deepcopy(B)\n",
    "    for j in range(n_batch):\n",
    "\n",
    "        ### Mini-batch creation\n",
    "        batch, one_hot_batch, mini_batch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "\n",
    "        prev_time = time.clock()\n",
    "\n",
    "        ### Forward propagation\n",
    "        Y, Yp = forward(act_func, W, B, batch)\n",
    "\n",
    "        ### Compute the softmax\n",
    "        out = softmax(Y[-1])\n",
    "        \n",
    "        ### Compute the gradient at the top layer\n",
    "        derror = out - one_hot_batch\n",
    "\n",
    "        ### Backpropagation\n",
    "        gradB = backward(derror, W, Yp)\n",
    "\n",
    "        ### Update the parameters\n",
    "        W, B = update(eta, batch_size, W, B, gradB, Y, regularizer, lamda)\n",
    "\n",
    "        exit()\n",
    "        curr_time = time.clock()\n",
    "        cumul_time += curr_time - prev_time\n",
    "\n",
    "    ### Training accuracy\n",
    "    train_loss, train_accuracy = computeLoss(act_func, W, B, train_set[0], train_set[1]) \n",
    "\n",
    "    ### Valid accuracy\n",
    "    valid_loss, valid_accuracy = computeLoss(act_func, W, B, valid_set[0], valid_set[1])\n",
    "    \n",
    "    if (train_loss<last_loss):\n",
    "        eta*=1.2\n",
    "    else:\n",
    "        eta*=0.8\n",
    "        W=W_copy\n",
    "        B=B_copy\n",
    "    last_loss=train_loss\n",
    "\n",
    "    g_i = np.append(g_i, i)\n",
    "    g_train_loss = np.append(g_train_loss, train_loss)\n",
    "    g_train_acc = np.append(g_train_acc, train_accuracy)\n",
    "    g_valid_loss = np.append(g_valid_loss, valid_loss)\n",
    "    g_valid_acc = np.append(g_valid_acc, valid_accuracy)\n",
    "    \n",
    "    print('{:02d}/{} {:.2f}s Tloss={:.4} Taccu={:.4} Vloss={:.4} Vaccu={:.4} Eta={}'.format(i+1, n_epoch, cumul_time, train_loss, train_accuracy, valid_loss, valid_accuracy, eta))\n",
    "    sys.stdout.flush() # Force emptying the stdout buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(g_i,g_train_loss,label='train_loss')\n",
    "plt.plot(g_i,g_valid_loss,label='valid_loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Negative log-likelihood\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(g_i,1.0-g_train_acc,label='train_acc')\n",
    "plt.plot(g_i,1.0-g_valid_acc,label='valid_acc')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Classification error\")\n",
    "plt.ylim([0.,1.])\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
