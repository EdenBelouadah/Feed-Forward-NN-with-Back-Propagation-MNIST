{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP2:\n",
    "\n",
    "Au cours du TP1, nous avons étudié le modèle *Softmax* pour traiter le problème de classification probabiliste. Le but était de présenter deux étapes importantes de l'entraînement : la forward propagation et la mise à jour des paramètres. Le TP2 reprend le modèle Softmax dans un cadre plus général, celui des réseaux de neurones avec couches cachèes.\n",
    "Dans ce cadre, on peut considérer le modèle Softmax comme un \"module\" qui prend en entrèe des \"features\", e.g. les pixels d'une image, et qui donne en sortie une loi de probabilité sur les étiquettes.\n",
    "Un réseau de neurones est composé de plusieurs modules, transformant simplement les features d'un espace à un autre en fonction des valeurs courantes des paramètres. Ainsi, le but de l'entraînement est d'apprendre les transformations pertinentes, i.e., en modifiant les paramètres, qui permettront de réaliser la tâche associée au module de sortie. En augmentant le nombre de modules (mais aussi de fonctions non-linéaires), on augmente ainsi la complexité du modèle.\n",
    "\n",
    "Le premier but du TP2 est de programmer les trois étapes essentielles à l'entraînement d'un réseau de neurones : la *forward propagation*, la *backpropagation* et la *mise à jour des paramètres*. Vérifiez que votre modèle fonctionne. Ensuite, vous pourrez comparer les performances de votre réseau de neurones avec celles de votre modèle Softmax de la semaine dernière.\n",
    "\n",
    "Une fois ces bases réalisées, on va pouvoir ajouter plusieurs fonctions d'activations en plus de la classique *sigmoïde*: les *tanh* et *relu*. Vous pourrez comparer la sigmoïde et la tanh avec la relu, notamment lorsque l'on utilise 2 couches cachées ou plus. Vous pourrez aussi mettre en évidence le phénomène de sur-apprentissage (travaillez avec une petite sous partie des données si nécessaire).\n",
    "Pour rappel, les fonctions sont:\n",
    "\n",
    "$$ tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n",
    "\n",
    "$$ relu(x) = max(0, x) $$\n",
    "\n",
    "Remarque: La fonction relu est plus instable numériquement que les deux autres. Il est possible qu'il soit nécessaire de réduire le taux d'apprentissage (ou de l'apapter, en le réduisant au fur et à mesure que l'apprentissage progresse) ou de forcer les valeurs de la relu à rester en dessous d'une limite que l'on choisit (*clipping*).\n",
    "\n",
    "Enfin, on va implémenter la *régularisation*: on ajoutera à la fonction de mise à jour des paramètres les méthodes de régularisation *L1* et *L2*. Il s'agira ensuite de vérifier leur influence sur les courbes d'apprentissage en faisant varier le paramètre $\\lambda$.\n",
    "\n",
    "A faire: \n",
    "- Compléter les fonctions:\n",
    "    - getDimDataset\n",
    "    - sigmoid\n",
    "    - forward\n",
    "    - backward\n",
    "    - update\n",
    "    - softmax\n",
    "    - computeLoss\n",
    "    - getMiniBatch\n",
    "- Compléter les fonctions:\n",
    "    - tanh\n",
    "    - relu\n",
    "    - et faire les expériences demandées.\n",
    "- Compléter les fonctions:\n",
    "    - updateParams\n",
    "    - et faire les expériences demandées.\n",
    "- Envoyer le notebook avec le code complété avant le **13 décembre 2017** à l'adresse **labeau@limsi.fr** accompagné d'un résumé d'un maximum de 6 pages contenant des figures et des analyses rapides des expériences demandées.\n",
    "- Le résumé doit être succinct et se focaliser uniquement sur les points essentiels reliés à l'entraînement des réseaux de neurones. En plus des résultats, ce document doit décrire les difficultés que vous avez rencontrées et, dans le cas échéant, les solutions utilisées pour les résoudre. Vous pouvez aussi y décrire vos questions ouvertes et proposer une expérience sur MNIST afin d'y répondre.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784, 10)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    !wget http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "\n",
    "#####################\n",
    "# Gestion des données\n",
    "#####################  \n",
    "import dataset_loader\n",
    "train_set, valid_set, test_set = dataset_loader.load_mnist()\n",
    "\n",
    "def getDimDataset(train_set):\n",
    "    n_training = len(train_set[0])\n",
    "    n_feature = len(train_set[0][0])\n",
    "    n_label = len(set(train_set[1]))\n",
    "    return n_training, n_feature, n_label\n",
    "\n",
    "n_training, n_feature, n_label = getDimDataset(train_set)\n",
    "print(n_training,n_feature,n_label)\n",
    "\n",
    "########################\n",
    "# Gestion des paramètres\n",
    "########################\n",
    "\n",
    "# Taille de la couche cachée: sous forme de liste, il est possible\n",
    "# d'utiliser plusieurs couches cachées, avec par exemple [128, 64]\n",
    "n_hidden =[300]\n",
    "\n",
    "# Fonction d'activation: à choisir parmi 'sigmoid', 'tanh' et 'relu'\n",
    "acti_fun = 'relu'\n",
    "act_func = 'relu'\n",
    "\n",
    "# Taille du batch\n",
    "batch_size = 500\n",
    "\n",
    "# Taux d'apprentissage:\n",
    "eta = 0.001\n",
    "\n",
    "# Nombre d'époques:\n",
    "n_epoch = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMiniBatch(i, batch_size, train_set, one_hot):\n",
    "    \"\"\"\n",
    "    Return a minibatch from the training set and the associated labels\n",
    "    Inputs: i: the identifier of the minibatch - int\n",
    "          : batch_size: the number of training examples - int\n",
    "          : train_set: the training set - ndarray\n",
    "          : one_hot: the one-hot representation of the labels - ndarray\n",
    "    Outputs: the minibatch of examples - ndarray\n",
    "           : the minibatch of labels - ndarray\n",
    "           : the number of examples in the minibatch - int\n",
    "    \"\"\"\n",
    "    idx_begin = i\n",
    "    idx_end = i+batch_size\n",
    "    batch = train_set[0][idx_begin:idx_end]\n",
    "    one_hot=one_hot.transpose()\n",
    "    one_hot_batch = one_hot[idx_begin:idx_end,:]\n",
    "    mini_batch_size = batch.shape[0]\n",
    "    return np.asfortranarray(batch), one_hot_batch, mini_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initNetwork(nn_arch, act_func_name):\n",
    "    \"\"\"\n",
    "    Initialize the neural network weights, activation function and return the number of parameters\n",
    "    Inputs: nn_arch: the number of units per hidden layer -  list of int\n",
    "          : act_func_name: the activation function name (sigmoid, tanh or relu) - str\n",
    "    Outputs: W: a list of weights for each hidden layer - list of ndarray\n",
    "           : B: a list of bias for each hidden layer - list of ndarray\n",
    "           : act_func: the activation function - function\n",
    "           : nb_params: the number of parameters  - int\n",
    "    \"\"\"\n",
    "    W,B = [],[]\n",
    "    sigma = 1.0\n",
    "    act_func = globals()[act_func_name] # Cast the string to a function\n",
    "    nb_params = 0\n",
    "\n",
    "    if act_func_name=='sigmoid':\n",
    "        sigma = 4.0\n",
    "\n",
    "    for i in range(np.size(nn_arch)-1):\n",
    "        w = np.random.normal(loc=0.0, scale=sigma/np.sqrt(nn_arch[i]), size=(nn_arch[i+1],nn_arch[i]))\n",
    "        W.append(w)\n",
    "        b = np.zeros((w.shape[0],1))\n",
    "        if act_func_name=='sigmoid':\n",
    "            b = np.sum(w,1).reshape(-1,1)/-2.0\n",
    "        B.append(b)\n",
    "        nb_params += nn_arch[i+1] * nn_arch[i] + nn_arch[i+1]\n",
    "\n",
    "    return W,B,act_func,nb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Fonctions d'activation\n",
    "########################\n",
    "\n",
    "def sigmoid(z, grad_flag=True):\n",
    "    \"\"\"\n",
    "    Perform the sigmoid transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - ndarray\n",
    "          : grad_flag: flag for computing the derivatives w.r.t. z - boolean\n",
    "    Outputs: y: the activation values - ndarray\n",
    "           : yp: the derivatives w.r.t. z - ndarray\n",
    "    \"\"\"\n",
    "    y=1.0/(1+np.exp(-1*z))\n",
    "    yp=y*(1-y)\n",
    "    return y, yp\n",
    "\n",
    "\n",
    "def tanh(z, grad_flag=True):\n",
    "    \"\"\"\n",
    "    Perform the tanh transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - ndarray\n",
    "          : grad_flag: flag for computing the derivatives w.r.t. z - boolean\n",
    "    Outputs: y: the activation values - ndarray\n",
    "    \"\"\"\n",
    "    y=(np.exp(z)-np.exp(-1*z))/(np.exp(z)+np.exp(-1*z))\n",
    "    yp=1-np.power(y,2)\n",
    "    return y, yp\n",
    "\n",
    "\n",
    "def relu(z, grad_flag=True):\n",
    "    \"\"\"\n",
    "    Perform the relu transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - ndarray\n",
    "          : grad_flag: flag for computing the derivatives w.r.t. z - boolean\n",
    "    Outputs: y: the activation values - ndarray\n",
    "    \"\"\"\n",
    "    y=np.maximum(0,z)\n",
    "    yp=np.zeros_like(y)\n",
    "    yp[y>0]=1\n",
    "    return y, yp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Création du réseau\n",
    "####################\n",
    "\n",
    "### Network Architecture\n",
    "nn_arch = np.array([n_feature] + n_hidden + [n_label])\n",
    "\n",
    "### Create the neural network\n",
    "W, B, act_func, nb_params = initNetwork(nn_arch, act_func)\n",
    "#for w in W:\n",
    "#        print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward(act_func, W, B, X):\n",
    "    \"\"\"\n",
    "    Perform the forward propagation\n",
    "    Inputs: act_func: the activation function - function\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias - list of ndarray\n",
    "          : X: the batch - ndarray\n",
    "    Outputs: Y: a list of activation values - list of ndarray\n",
    "           : Yp: a list of the derivatives w.r.t. the pre-activation of the activation values - list of ndarray\n",
    "    \"\"\"\n",
    "    a=X\n",
    "    Yp=[]\n",
    "    Y=[a]\n",
    "    for i in range(len(W)-1):\n",
    "        z=(a.dot(W[i].transpose()))+B[i].transpose()\n",
    "        y,yp=act_func(z)\n",
    "        Y.append(y)\n",
    "        Yp.append(yp)\n",
    "        a=y\n",
    "    \n",
    "    z=(a.dot(W[-1].transpose()))+B[-1].transpose()\n",
    "    Y.append(z)    \n",
    "    return Y, Yp\n",
    "#W, B, act_func, nb_params = initNetwork(nn_arch, \"sigmoid\")\n",
    "#batch=train_set[0][0:5]\n",
    "#one_hot = np.zeros((n_label,n_training))\n",
    "#one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "#batch,one_hot_batch, mini_batch_size=getMiniBatch(0, 5, train_set, one_hot)\n",
    "#Y, Yp=forward(act_func, W, B, batch)\n",
    "#print([y.shape for y in Y])\n",
    "#print([y.shape for y in Yp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "        Perform the softmax transformation to the pre-activation values\n",
    "        :param z: the pre-activation values\n",
    "        :type z: ndarray\n",
    "        :return: the activation values\n",
    "        :rtype: ndarray\n",
    "    \"\"\"\n",
    "    exps = np.exp(z-np.max(z,axis=1).reshape(-1,1))\n",
    "    somme_exps = np.sum(exps,axis=1)\n",
    "    for i in range(somme_exps.shape[0]):\n",
    "        exps[i]/=somme_exps[i]\n",
    "    return exps\n",
    "#out=softmax(z)\n",
    "#print(out.shape)\n",
    "#print (out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def backward(error, W, Yp): \n",
    "    \"\"\"\n",
    "    Perform the backward propagation\n",
    "    Inputs: error: the gradient w.r.t. to the last layer - ndarray\n",
    "          : W: the weights - list of ndarray\n",
    "          : Yp: the derivatives w.r.t. the pre-activation of the activation functions - list of ndarray\n",
    "    Outputs: gradb: a list of gradient w.r.t. the pre-activation with this order [gradb_layer1, ..., error] - list of ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    gradB = [error]\n",
    "    for w, yp in zip(reversed(W), reversed(Yp)):\n",
    "        error = error.dot(w) * yp\n",
    "        gradB.append(error)\n",
    "    gradB.reverse()\n",
    "    return gradB\n",
    "\n",
    "#out = softmax(Y[-1])\n",
    "### Compute the gradient at the top layer\n",
    "#derror = out - one_hot_batch\n",
    "#gradB=backward(derror, W, Yp)\n",
    "#for b in gradB: print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateParams(theta, dtheta, eta, regularizer=None, lamda=0.):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: theta: the network parameters - ndarray w\n",
    "          : dtheta: the updates of the parameters - ndarray\n",
    "          : eta: the step-size of the gradient descent - float\n",
    "          : regularizer: choice of the regularizer: None, 'L1', or 'L2'\n",
    "          : lambda: hyperparamater giving the importance of the regularizer - float\n",
    "    Outputs: the parameters updated - ndarray\n",
    "    \"\"\"\n",
    "    theta=np.copy(theta)\n",
    "    if regularizer=='L1':\n",
    "        theta+=lamda*np.abs(theta)\n",
    "    elif regularizer=='L2':\n",
    "        theta+=lamda*np.power(theta,2)\n",
    "    return theta - eta * dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(eta, batch_size, W, B, gradB, Y, regularizer, lamda):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: eta: the step-size of the gradient descent - float \n",
    "          : batch_size: number of examples in the batch (for normalizing) - int\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias -  list of ndarray\n",
    "          : gradB: the gradient of the activations w.r.t. to the loss -  list of ndarray\n",
    "          : Y: the activation values -  list of ndarray\n",
    "    Outputs: W: the weights updated -  list of ndarray\n",
    "           : B: the bias updated -  list of ndarray\n",
    "    \"\"\"\n",
    "    # grad_b should be a vector: object.reshape(-1,1) can be useful\n",
    "    \n",
    "   #accumulated_gradients_W = [np.zeros_like(w) for w in W]\n",
    "    #for i in range(len(W)):\n",
    "    #    for x in range(batch_size):\n",
    "    #        accumulated_gradients_W[i] += (gradB[i][x].reshape(-1,1)).transpose().dot(Y[i+1][x])\n",
    "    #    W[i]=updateParams(W[i], accumulated_gradients_W[i]/batch_size, eta,regularizer,lamda)\n",
    "    #    B[i]=updateParams(B[i], np.sum(gradB[i], axis=0, keepdims=True).transpose()/batch_size, eta,regularizer,lamda)\n",
    "\n",
    "    #return W, B#\n",
    "    k=len(W)-1\n",
    "    while(k>0):\n",
    "        grad_w=((gradB[k].transpose()).dot(Y[k]))/batch_size\n",
    "        W[k]=(1-eta * lamda/batch_size)*W[k]\n",
    "        W[k]=updateParams(W[k],grad_w, eta,regularizer,lamda)\n",
    "        grad_b = (np.sum(gradB[k],1).reshape(-1,1))/batch_size\n",
    "        B[k]=updateParams(B[k],grad_b[0], eta,regularizer,lamda)\n",
    "        k-=1\n",
    "        \n",
    "    return W,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeLoss(act_func, W, B, X, labels):\n",
    "    \"\"\"\n",
    "    Compute the loss value of the current network on the full batch\n",
    "    Inputs: act_func: the activation function - function\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias - list of ndarray\n",
    "          : X: the batch - ndarray\n",
    "          : labels: the labels corresponding to the batch\n",
    "    Outputs: loss: the negative log-likelihood - float\n",
    "           : accuracy: the ratio of examples that are well-classified - float\n",
    "    \"\"\" \n",
    "    ### Forward propagation\n",
    "    Y, Yp = forward(act_func, W, B, X)\n",
    " \n",
    "    ### Compute the softmax and the prediction\n",
    "    out = softmax(Y[-1])\n",
    "    #pred = np.argmax(out,axis=0)\n",
    "    \n",
    "    #Loss and Accuracy    \n",
    "    loss = -np.sum(np.log(out[np.arange(out.shape[0]), labels]))/labels.shape[0]\n",
    "    accuracy = np.mean((np.argmax(out, axis=1) == labels).astype(np.float, copy=False))\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture = [784 300  10] | Batch size = 500 | Eta = 0.001 | Act Func=relu\n",
      "01/45 2.11s Tloss=2.312 Taccu=10.97% Vloss=2.309 Vaccu=10.94% Eta=0.0012\n",
      "02/45 3.72s Tloss=2.294 Taccu=12.78% Vloss=2.291 Vaccu=12.84% Eta=0.00144\n",
      "03/45 5.31s Tloss=2.274 Taccu=14.92% Vloss=2.27 Vaccu=14.88% Eta=0.001728\n",
      "04/45 6.96s Tloss=2.25 Taccu=17.89% Vloss=2.246 Vaccu=17.88% Eta=0.0020736\n",
      "05/45 9.08s Tloss=2.223 Taccu=21.38% Vloss=2.219 Vaccu=21.56% Eta=0.00248832\n",
      "06/45 10.99s Tloss=2.193 Taccu=25.22% Vloss=2.187 Vaccu=26.06% Eta=0.002985984\n",
      "07/45 12.92s Tloss=2.157 Taccu=29.37% Vloss=2.151 Vaccu=31.08% Eta=0.0035831808\n",
      "08/45 14.87s Tloss=2.116 Taccu=34.74% Vloss=2.109 Vaccu=36.66% Eta=0.00429981696\n",
      "09/45 16.79s Tloss=2.069 Taccu=41.0% Vloss=2.061 Vaccu=42.93% Eta=0.005159780352\n",
      "10/45 18.66s Tloss=2.016 Taccu=47.98% Vloss=2.006 Vaccu=50.0% Eta=0.0061917364224\n",
      "11/45 20.50s Tloss=1.954 Taccu=53.6% Vloss=1.943 Vaccu=55.37% Eta=0.00743008370688\n",
      "12/45 22.33s Tloss=1.885 Taccu=57.91% Vloss=1.872 Vaccu=59.96% Eta=0.00891610044826\n",
      "13/45 24.05s Tloss=1.809 Taccu=61.96% Vloss=1.793 Vaccu=64.28% Eta=0.0106993205379\n",
      "14/45 25.83s Tloss=1.725 Taccu=65.64% Vloss=1.707 Vaccu=67.74% Eta=0.0128391846455\n",
      "15/45 27.65s Tloss=1.636 Taccu=68.54% Vloss=1.615 Vaccu=70.72% Eta=0.0154070215746\n",
      "16/45 29.41s Tloss=1.543 Taccu=70.92% Vloss=1.519 Vaccu=73.19% Eta=0.0184884258895\n",
      "17/45 31.24s Tloss=1.447 Taccu=72.9% Vloss=1.42 Vaccu=75.31% Eta=0.0221861110674\n",
      "18/45 33.16s Tloss=1.352 Taccu=74.36% Vloss=1.321 Vaccu=76.84% Eta=0.0266233332809\n",
      "19/45 35.03s Tloss=1.259 Taccu=75.68% Vloss=1.225 Vaccu=78.28% Eta=0.0319479999371\n",
      "20/45 36.92s Tloss=1.17 Taccu=76.64% Vloss=1.132 Vaccu=79.09% Eta=0.0383375999245\n",
      "21/45 38.81s Tloss=1.087 Taccu=77.57% Vloss=1.046 Vaccu=79.91% Eta=0.0460051199094\n",
      "22/45 40.63s Tloss=1.01 Taccu=78.31% Vloss=0.9675 Vaccu=80.63% Eta=0.0552061438912\n",
      "23/45 42.46s Tloss=0.9407 Taccu=78.99% Vloss=0.896 Vaccu=81.15% Eta=0.0662473726695\n",
      "24/45 44.29s Tloss=0.8784 Taccu=79.72% Vloss=0.8321 Vaccu=81.77% Eta=0.0794968472034\n",
      "25/45 45.89s Tloss=0.823 Taccu=80.31% Vloss=0.7754 Vaccu=82.46% Eta=0.0953962166441\n",
      "26/45 47.58s Tloss=0.774 Taccu=80.92% Vloss=0.7256 Vaccu=83.05% Eta=0.114475459973\n",
      "27/45 49.23s Tloss=0.731 Taccu=81.43% Vloss=0.682 Vaccu=83.44% Eta=0.137370551967\n",
      "28/45 50.83s Tloss=0.6936 Taccu=81.88% Vloss=0.6442 Vaccu=83.89% Eta=0.164844662361\n",
      "29/45 52.47s Tloss=0.6611 Taccu=82.3% Vloss=0.6115 Vaccu=84.3% Eta=0.197813594833\n",
      "30/45 54.10s Tloss=0.6332 Taccu=82.54% Vloss=0.5835 Vaccu=84.5% Eta=0.2373763138\n",
      "31/45 55.76s Tloss=0.6094 Taccu=82.79% Vloss=0.5598 Vaccu=84.65% Eta=0.28485157656\n",
      "32/45 57.57s Tloss=0.5894 Taccu=83.0% Vloss=0.5398 Vaccu=84.77% Eta=0.341821891872\n",
      "33/45 59.19s Tloss=0.5729 Taccu=83.21% Vloss=0.5232 Vaccu=84.79% Eta=0.410186270246\n",
      "34/45 60.81s Tloss=0.5595 Taccu=83.39% Vloss=0.5097 Vaccu=84.84% Eta=0.492223524295\n",
      "35/45 62.44s Tloss=0.549 Taccu=83.49% Vloss=0.4989 Vaccu=84.84% Eta=0.590668229154\n",
      "36/45 64.00s Tloss=0.5411 Taccu=83.56% Vloss=0.4906 Vaccu=84.85% Eta=0.708801874985\n",
      "37/45 65.62s Tloss=0.5356 Taccu=83.61% Vloss=0.4846 Vaccu=84.95% Eta=0.850562249982\n",
      "38/45 67.24s Tloss=0.5322 Taccu=83.65% Vloss=0.4806 Vaccu=84.91% Eta=1.02067469998\n",
      "39/45 68.87s Tloss=0.5308 Taccu=83.66% Vloss=0.4784 Vaccu=84.93% Eta=1.22480963997\n",
      "40/45 70.48s Tloss=0.5312 Taccu=83.67% Vloss=0.4778 Vaccu=84.93% Eta=1.10232867598\n",
      "41/45 72.08s Tloss=0.531 Taccu=83.69% Vloss=0.4777 Vaccu=84.94% Eta=1.32279441117\n",
      "42/45 73.75s Tloss=0.5326 Taccu=83.66% Vloss=0.4783 Vaccu=85.02% Eta=1.19051497005\n",
      "43/45 75.37s Tloss=0.5322 Taccu=83.68% Vloss=0.4781 Vaccu=85.02% Eta=1.42861796407\n",
      "44/45 77.00s Tloss=0.5348 Taccu=83.68% Vloss=0.4797 Vaccu=85.02% Eta=1.28575616766\n",
      "45/45 78.59s Tloss=0.5344 Taccu=83.68% Vloss=0.4794 Vaccu=84.99% Eta=1.54290740119\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# Data structures for plotting\n",
    "g_i = []\n",
    "g_train_loss=[]\n",
    "g_train_acc=[]\n",
    "g_valid_loss=[]\n",
    "g_valid_acc=[]\n",
    "\n",
    "#############################\n",
    "### Auxiliary variables\n",
    "#############################\n",
    "cumul_time = 0.\n",
    "n_batch = int(math.ceil(float(n_training)/batch_size))\n",
    "regularizer = None\n",
    "lamda = 0.\n",
    "last_loss=np.inf\n",
    "\n",
    "# Convert the labels to one-hot vector\n",
    "one_hot = np.zeros((n_label,n_training))\n",
    "one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "\n",
    "print('Architecture = {} | Batch size = {} | Eta = {} | Act Func={}'.format(nn_arch, batch_size, eta, acti_fun))\n",
    "\n",
    "#############################\n",
    "### Learning process\n",
    "#############################\n",
    "for i in range(n_epoch):\n",
    "    W_copy=copy.deepcopy(W)\n",
    "    B_copy=copy.deepcopy(B)\n",
    "    for j in range(n_batch):\n",
    "\n",
    "        ### Mini-batch creation\n",
    "        batch, one_hot_batch, mini_batch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "\n",
    "        prev_time = time.clock()\n",
    "\n",
    "        ### Forward propagation\n",
    "        Y, Yp = forward(act_func, W, B, batch)\n",
    "\n",
    "        ### Compute the softmax\n",
    "        out = softmax(Y[-1])\n",
    "        \n",
    "        ### Compute the gradient at the top layer\n",
    "        derror = out - one_hot_batch\n",
    "\n",
    "        ### Backpropagation\n",
    "        gradB = backward(derror, W, Yp)\n",
    "\n",
    "        ### Update the parameters\n",
    "        W, B = update(eta, batch_size, W, B, gradB, Y, regularizer, lamda)\n",
    "\n",
    "        curr_time = time.clock()\n",
    "        cumul_time += curr_time - prev_time\n",
    "\n",
    "    ### Training accuracy\n",
    "    train_loss, train_accuracy = computeLoss(act_func, W, B, train_set[0], train_set[1]) \n",
    "\n",
    "    ### Valid accuracy\n",
    "    valid_loss, valid_accuracy = computeLoss(act_func, W, B, valid_set[0], valid_set[1])\n",
    "    \n",
    "    if (train_loss<last_loss):\n",
    "        eta*=1.2\n",
    "    else:\n",
    "        eta*=0.9\n",
    "        W=W_copy\n",
    "        B=B_copy\n",
    "    last_loss=train_loss\n",
    "\n",
    "    g_i = np.append(g_i, i)\n",
    "    g_train_loss = np.append(g_train_loss, train_loss)\n",
    "    g_train_acc = np.append(g_train_acc, train_accuracy)\n",
    "    g_valid_loss = np.append(g_valid_loss, valid_loss)\n",
    "    g_valid_acc = np.append(g_valid_acc, valid_accuracy)\n",
    "    print('{:02d}/{} {:.2f}s Tloss={:.4} Taccu={:.4}% Vloss={:.4} Vaccu={:.4}% Eta={}'.format(i+1, n_epoch, cumul_time, train_loss, train_accuracy*100, valid_loss, valid_accuracy*100, eta))\n",
    "    sys.stdout.flush() # Force emptying the stdout buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xa84cfd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Wd0VPXWgPFnTzodQuhEQHoNEIpSxA6IgkizoGBBEAuK\nKPbyei3XdvVepKgUUVFAUEQUFCkCioTeqyABhNATSCBlvx/OASMkYYBMJmX/1jprTp3ZOQtmz/lX\nUVWMMcaYc/H4OwBjjDF5gyUMY4wxXrGEYYwxxiuWMIwxxnjFEoYxxhivWMIwxhjjFUsYxhhjvGIJ\nwxhjjFcsYRhjjPFKoL8DyE6lS5fWKlWq+DsMY4zJM5YuXbpfVSO8OTdfJYwqVaoQExPj7zCMMSbP\nEJEd3p5rRVLGGGO8YgnDGGOMVyxhGGOM8Uq+qsMwxuQvycnJxMbGkpSU5O9Q8rzQ0FAqVapEUFDQ\nBb+HJQxjTK4VGxtL0aJFqVKlCiLi73DyLFXlwIEDxMbGUrVq1Qt+HyuSMsbkWklJSYSHh1uyuEgi\nQnh4+EU/qVnCMMbkapYsskd23EdLGMDEqVOYEbOZQ8dO+jsUY4zJtQp8HUZSUiKdVg4gcEUyMdNq\nsaloSzw1rqVeVEuiIksS4LFfN8YYA/aEQWhwMCF3TuJgw3upUeQkfY6N5s4Vt1J+TFO+frkro0b+\nh29+38iR48n+DtUYk8MOHz7MBx98cN7XdezYkcOHD5/3dX369GHy5MnnfV1OKfBPGHgCCKjWlnLV\n2jrbR3dzfN0PyKrv6fjXQsL2/EzS7lf4ZXojtpS+ivAmnWnXqDpliob6N25jjM+dShgPPPDAP/an\npKQQGJj51+eMGTN8HZpf+CxhiEhl4BOgLKDAKFV974xzbgeeBASIBwao6kr32HZ3XyqQoqrRvor1\nH4pVoFDLuynU8m5ITSZtx28cjZlMi83fce3Bf3Pyx7dZOLM+k0teSdGozlzbtC7lilvyMMbXXvp2\nLet2H83W96xboRgv3Fgv0+NDhw5l69atREVFERQURGhoKCVLlmTDhg1s2rSJLl26sHPnTpKSknjk\nkUfo168f8Pe4dgkJCXTo0IHWrVuzaNEiKlasyDfffENYWNg5Y5s9ezaPP/44KSkpNGvWjOHDhxMS\nEsLQoUOZNm0agYGBXHfddbz11ltMmjSJl156iYCAAIoXL878+fOz7R6l58snjBRgsKouE5GiwFIR\n+VFV16U75w/gClU9JCIdgFFAi3THr1TV/T6MMWsBQXiqtaFMtTaQ9i66K4aEmMk02fAtVx79Dynz\n3mfenEZ8GXEDkS1v4bpGkRQOsYc2Y/KL119/nTVr1rBixQrmzp3LDTfcwJo1a073ZRg9ejSlSpUi\nMTGRZs2accsttxAeHv6P99i8eTMTJkzgww8/pEePHnz11VfccccdWX5uUlISffr0Yfbs2dSsWZM7\n77yT4cOH07t3b6ZOncqGDRsQkdPFXi+//DIzZ86kYsWKF1QU5i2ffbup6h5gj7seLyLrgYrAunTn\nLEp3yW9AJV/Fc9E8HqRyc0pVbg76BuxZwbGYibRYM4mrD77C4e/e5ZvprdhbrSvRl1/N5dUjrMLc\nmGyU1ZNATmnevPk/Or69//77TJ06FYCdO3eyefPmsxJG1apViYqKAqBp06Zs3779nJ+zceNGqlat\nSs2aNQG46667GDZsGA8++CChoaHcc889dOrUiU6dOgHQqlUr+vTpQ48ePejatWt2/KkZypFKbxGp\nAjQGFmdx2j3A9+m2FfhJRJaKSD/fRXcBRKBCY4rf9BpFhm5Eb/+K1GpX0z1gLo9u70+5T9vxwb8e\nYviMxfx1xIY0MCa/KFy48On1uXPn8tNPP/Hrr7+ycuVKGjdunGHHuJCQkNPrAQEBpKSkXPDnBwYG\n8vvvv9OtWzemT59O+/btARgxYgSvvPIKO3fupGnTphw4cOCCPyPLz/fJu6YjIkWAr4BBqpphAaSI\nXImTMFqn291aVXeJSBngRxHZoKpnFcy5yaQfQGRkZLbHf06eAKTGNYTXuAaSjpC86ivCF3/CQwfG\nc2LxBKb/ejmbqtxGu3bX0bJaKeuEZEweUrRoUeLj4zM8duTIEUqWLEmhQoXYsGEDv/32W7Z9bq1a\ntdi+fTtbtmyhevXqjB8/niuuuIKEhASOHz9Ox44dadWqFdWqVQNg69attGjRghYtWvD999+zc+fO\ns550soNPE4aIBOEki89UdUom5zQEPgI6qOrptKiqu9zXfSIyFWgOnJUwVHUUTt0H0dHRmu1/xPkI\nLU5Q87sp1fxuiNvIyfkfcOPaLwneOZ+YcTV5rfBNRLbuRZfoqhSxug5jcr3w8HBatWpF/fr1CQsL\no2zZsqePtW/fnhEjRlCnTh1q1apFy5Yts+1zQ0NDGTNmDN27dz9d6d2/f38OHjxI586dSUpKQlV5\n5513ABgyZAibN29GVbn66qtp1KhRtsWSnqj65jtWnJ/S44CDqjook3MigZ+BO9PXZ4hIYcDj1n0U\nBn4EXlbVH7L6zOjoaM11M+4lHSF56XiSFo6g6PGd7NUSTOQ6UqPv5fZ2UUQUDTn3exhTQK1fv546\nder4O4x8I6P7KSJLvW2F6sufua2A3sBqEVnh7nsaiARQ1RHA80A48IFbVHOq+WxZYKq7LxD4/FzJ\nItcKLU5QqwcJuuwBdPMswuYP46FdE4mP+ZbPllzH0aj7ueOqJlQoce5mdsYY40++bCW1AKd/RVbn\n3Avcm8H+bYBvnqn8xeNBarWnWK32sHct8uNr9NsyjcSVP/D58mv5q9599L6mGVVKFz73exlj8rSB\nAweycOHCf+x75JFH6Nu3r58i8o4VpPtD2XoUueNTiNuIzH6DezZM5cT6WXy+5mq2176XeztcxiXh\nljiMya+GDRvm7xAuSIEfS8qvImpRqNdoPA8ugfo30zdwFs9s7sV3/3mA179eYqPnGmNyFUsYuUHp\n6oR1H4Xn4aVQ5yYeCPiau5d34903n2PU3E0kJaf6O0JjjLGEkauUqkpor9Fw72yKlq/Oy4yg9c/d\nGPLv//LNil2kpfm31bAxpmCzhJEbVYom7P6foNtoqhVN4b/JL1Doq94M/O9E1uw64u/ojDEFlCWM\n3EoE6t9C6KBlpF31PO2C1/P+oQf4cfhjvD59FcdPXvjwAsYY3yhSpAgAu3fvplu3bhme065dO7Lq\nL1alShX27/ffmKtZsYSR2wWF4mk7mKBBK9A6N/Jo4GQ6/34bD789mvmb4vwdnTEmAxUqVMjVEyFd\nKGtWm1cULUtwz7GwoQeXfjOIkYlP8uEn85he72GevDGK8CLWY9zkc98Phb9WZ+97lmsAHV7P9PDQ\noUOpXLkyAwcOBODFF18kMDCQOXPmcOjQIZKTk3nllVfo3LnzP67bvn07nTp1Ys2aNSQmJtK3b19W\nrlxJ7dq1SUxM9Dq8d955h9GjRwNw7733MmjQII4dO0aPHj2IjY0lNTWV5557jp49e2Y4T0Z2s4SR\n19TuSPAll5My81n6rxjPHxtieGLTADrd1I0uURVtcENjslHPnj0ZNGjQ6YQxceJEZs6cycMPP0yx\nYsXYv38/LVu25Kabbsr0/97w4cMpVKgQ69evZ9WqVTRp0sSrz166dCljxoxh8eLFqCotWrTgiiuu\nYNu2bVSoUIHvvvsOcAZBPHDgQIbzZGQ3Sxh5UVgJArv8Dxp2o+LUh/g4/gXGfbWAx9c9zou3NKVo\naJC/IzQm+2XxJOArjRs3Zt++fezevZu4uDhKlixJuXLlePTRR5k/fz4ej4ddu3axd+9eypUrl+F7\nzJ8/n4cffhiAhg0b0rBhQ68+e8GCBdx8882nh1Tv2rUrv/zyC+3bt2fw4ME8+eSTdOrUiTZt2pCS\nkpLhPBnZzeow8rJq7Qh+6DfSWgzgrsAfuXfjffT/z5es3Om7GbeMKWi6d+/O5MmT+fLLL+nZsyef\nffYZcXFxLF26lBUrVlC2bNkM58HwlZo1a7Js2TIaNGjAs88+y8svv5zpPBnZzRJGXhdcGE+H1+H2\nydQIi+fDpMGMGfkmo+ZvtX4bxmSDnj178sUXXzB58mS6d+/OkSNHKFOmDEFBQcyZM4cdO3ZkeX3b\ntm35/PPPAVizZg2rVq3y6nPbtGnD119/zfHjxzl27BhTp06lTZs27N69m0KFCnHHHXcwZMgQli1b\nRkJCAkeOHKFjx468++67rFy58qL/7oxYkVR+UeNaAh9YCBP78p9d/+PzWeu4b9NjvN6zuQ2hbsxF\nqFevHvHx8VSsWJHy5ctz++23c+ONN9KgQQOio6OpXbt2ltcPGDCAvn37UqdOHerUqUPTpk29+twm\nTZrQp08fmjdvDjiV3o0bN2bmzJkMGTIEj8dDUFAQw4cPJz4+PsN5MrKbz+bD8IdcOR9GTktNRn/+\nF7LwXTZoJM8EPs6jvW6gdY3S/o7MmPNm82Fkr4udD8OKpPKbgCDk2hfh9slUD4vn09QnmDT2XUYv\n+IP89OPAGJPzLGHkVzWuJXDAQoIrNeK9oP8R/8PLPDN1Ncmpaf6OzBgDtGjRgqioqH8sq1dncz+T\nbGZ1GPlZ8YoE9JmOfvsIj6z8nKnL93LP/qG8f0dLShQK9nd0xnhFVfNl/6LFixfn6OdlRwmDPWHk\nd4HBSJcP4KrnuDlgIQ/FDuGuYT+wLS7B35EZc06hoaEcOHDAilMvkqpy4MABQkNDL+p9fFbpLSKV\ngU9w5udWYJSqvnfGOQK8B3QEjgN9VHWZe6y9eywA+EhVz9lrxyq9z2H1ZNKmDmBnWjgP8BRP33ED\nrapbZbjJvZKTk4mNjc3Rfg75VWhoKJUqVSIo6J8de8+n0tuXRVIpwGBVXSYiRYGlIvKjqq5Ld04H\noIa7tACGAy1EJAAYBlwLxAJLRGTaGdea89WgG57ilaj0eS8+P/EM9405wJ+du3Fr80h/R2ZMhoKC\ngqhataq/wzAunxVJqeqeU08LqhoPrAcqnnFaZ+ATdfwGlBCR8kBzYIuqblPVk8AX7rnmYkW2JOC+\n2RQtEcFnQa+y6OuRjJq/1d9RGWPygBypwxCRKkBj4MxanorAznTbse6+zPab7BB+KZ77ZhMQGc17\nwcPYPHME7/64ycqJjTFZ8nnCEJEiwFfAIFU96oP37yciMSISExdn80N4rVApPL2nItWu5M2gUeyb\nO4JXZ6y3pGGMyZRPE4aIBOEki89UdUoGp+wCKqfbruTuy2z/WVR1lKpGq2p0RERE9gReUASFIbdO\nQKtfx2tBH5O0aCTPfr3GxqAyxmTIZwnDbQH1MbBeVTMb2GQacKc4WgJHVHUPsASoISJVRSQY6OWe\na7JbUCjS61O0Vgf+L2gswTEjeXzSSlKsg58x5gy+fMJoBfQGrhKRFe7SUUT6i0h/95wZwDZgC/Ah\n8ACAqqYADwIzcSrLJ6rqWh/GWrAFhiDdP0Hr3MgLQeMJXzWSh79YzskUSxrGmL/Z4IPmb6nJMOU+\nWDuVfyf3ZFPNfgy/owlBAda/05j8ygYfNBcmIAi6fgQNevBE0JfU2jSSJyavsjoNYwxgCcOcKSAQ\nbh4BDXsxJGgiYas+4eXp66z1lDHGEobJgCcAOv8PrXE9rwSNYc9vE3n3p83+jsoY42eWMEzGAoKQ\n7mORSk35X8gHLP55Gh8v+MPfURlj/CjThCEipbJacjJI4yfBhZDbJhIYXpWxYe8w+bvvmbw01t9R\nGWP8JKvBB5fijDIrQCRwyF0vAfwJ2IhgBUGhUkjvKYR+dC0TeJPOXxWiaGh7rq9Xzt+RGWNyWKZP\nGKpaVVWrAT8BN6pqaVUNBzoBs3IqQJMLFK+E9J5C8eA0Pg97g+c/n8eiLfv9HZUxJod5U4fRUlVn\nnNpQ1e+By30XksmVytRBbptIBTnEJ6H/ZtCnC9myzyZhMqYg8SZh7BaRZ0Wkirs8A+z2dWAmF4ps\ngXQfQ820P3iH/9Bv7G8cOnbS31EZY3KINwnjViACmOouZdx9piCq1QHp+CatWUav+HEM+GypDSFi\nTAFxzhn3VPUg8Ig7a56qqpVDFHTN7oG9a+gXM5o12yvz/DeFea1rA5zxJo0x+dU5nzBEpIGILAfW\nAGtFZKmI1Pd9aCZXa/8GRF7O2yEfsSZmnvXRMKYA8KZIaiTwmKpeoqqXAIOBUb4Ny+R6gcHQ4xMC\ni0UwvvB7jJzxK7PX7/V3VMYYH/ImYRRW1TmnNlR1LlDYZxGZvKNIBNJrAiXkGOMK/5fBE35n/Z5s\nn1TRGJNLeJMwtonIc+laST2LM4eFMVC+IdLlA+qmrOelwDHcO3YJcfEn/B2VMcYHvEkYd+O0kpri\nLhHuPmMc9W6GtkPonDabDonTGPj5Mpuxz5h86JwJQ1UPqerDwBVAW1V9RFUP+T40k6e0expqdeTp\ngPEE7PiFN2du9HdExphsZq2kTPbweODmkXjCqzOq0HCmzF/GzLV/+TsqY0w28lkrKREZLSL7RGRN\nJseHpJvre42IpJ4aBVdEtovIaveYzbmaV4QWgx7jKEIiHxcdwRMTl7N9/zF/R2WMySa+bCU1Fmif\n2UFVfVNVo1Q1CngKmOd2EjzlSve4V3PNmlyiTB2k45s0TF7F/TKFAZ8tIyk51d9RGWOygc9aSanq\nfODguc5z3QpM8PJck9s1vgMa9mIAkyix91ee+zrDh0xjTB7j91ZSIlII50nkq3S7FfjJrS/pl12f\nZXKICNzwNlK6Bh8WHsGcpWuZuGSnv6Myxlwkb8aSOgQ87MMYbgQWnlEc1VpVd4lIGeBHEdngPrGc\nxU0o/QAiIyN9GKY5LyFFoPtYCn94FWOKj6LHN8WpV7EY9SoU93dkxpgL5E0rqZoiMkpEZonIz6eW\nbIyhF2cUR6nqLvd1H84Iuc0zu1hVR6lqtKpGR0REZGNY5qKVrYd0eIMGJ5YzKORbBny6jCOJyf6O\nyhhzgbwpkpoELAeeBYakWy6aiBTH6d/xTbp9hd2RcRGRwsB1OE16TV7U5C6o341+aV9S6chShn61\nClX1d1TGmAtwziIpIEVVh5/vG4vIBKAdUFpEYoEXgCAAVR3hnnYzMEtV07e9LAtMdYfKDgQ+V9Uf\nzvfzTS4hAjf+B9m9nA8TRtB2TQUmxkTQs5kVHxqT10hmv/ZO9YnAqb84VTR0epCgM+occoXo6GiN\nibFuG7nSnlXoR9ewPCiK2489yvSH23BpRBF/R2VMgSciS73tvpBVkdRSIAa4C6cIapG779R+Y7xX\nviFyzYs0SVrMrYFzeHjCck6kWP8MY/KSTBOGqlZV1Wru65lLtZwM0uQTLfpD1St4OmA8CXs28fas\nTf6OyBhzHjKtwxCRq1T1ZxHpmtFxVZ3iu7BMvuTxQJfhBA6/jPElP+bK+RG0rl6atjWtdZsxeUFW\nRVJXuK83ZrB08nFcJr8qXhFueIfI42t5rvj3PDZxJfsTbP4MY/KCTJ8wVPUF97VvzoVjCoQG3WDj\n99y19ku+Ta7HE5OL8/Fd0bgt44wxuVRWRVKPZXWhqr6T/eGYAuOGt5A/f2V0yoe02FCBT36N4K7L\nq/g7KmNMFrIqkip6jsWYCxdWErp8QPHj2xkW8TX/mrGeDX/ZfODG5GZZFUm9lJOBmAKoWjto+QBX\n//YB1wc34NEvi/DNwFYEB3ozAIExJqd5O5bU7FMTIYlIQ3eIc2Mu3tXPQ0Rt3goeyV97YnlvtjW1\nNSa38uan3Ic4ExwlA6jqKpwBA425eEFh0PVDQk4eZkzZSQyfu5WlO2zKeGNyI28SRiFV/f2MfSm+\nCMYUUOUbQtshRB2ZTa+iKxg8cQXHT9o/MWNyG28Sxn4RuRRnUiNEpBuwx6dRmYKnzWNQrgEvej7m\nyMG9vDZjg78jMsacwZuEMRAYCdQWkV3AIGCAT6MyBU9AEHQZTvDJI4wv/xXjf9vBvE1x/o7KGJOO\nNwljl6pegzM1a21VbQ1Y+0eT/co1gLZDqH9wFneVXMMTk1dy5LhNuGRMbuFNwpgiIoGqekxV40Wk\nHPCjrwMzBVTrx6BsA57lQ1ISDvL8NJs7y5jcwpuE8TUwSUQCRKQKMAun1ZQx2S8wGLp8QNCJQ3xW\ncQrfrNjN9FW7/R2VMQYvEoaqfgj8hJM4vgX6q+osXwdmCrDyDaHN49SO+577yqzn2a/XsO9okr+j\nMqbAyzRhiMhjpxYgFIgEVgAtzzXOlDEXrc1gKFufJ1NGEnzyCE9PXW1zgRvjZ96OJVUEmAJswcux\npERktIjsO9VDPIPj7UTkiIiscJfn0x1rLyIbRWSLiAw9nz/I5BNu0VRg0kE+qzSVn9bvY8qyXf6O\nypgCzZdjSY0F/gd8ksU5v6jqP+bWEJEAYBhwLRALLBGRaaq67iLjMXlN+UbQZjA15r3B/eWa8eK3\ngbSqXppyxUP9HZkxBVJWRVL/cV+/FZFpZy7nemNVnQ8cvICYmgNbVHWbqp4EvgA6X8D7mPygzeNQ\npi5DkkcSkprA0CmrrGjKGD/J9AkDGO++vuXDz79cRFYBu4DHVXUtUBHYme6cWKCFD2MwuVlgMNz0\nPwI/vobPImdw/cauTIqJpUezyv6OzJgCJ6siqaXu6zwfffYyIFJVE0SkI04rrBrn+yYi0g/oBxAZ\nGZm9EZrcoVJTaPkAtX79H30rNOP/pgfSqkZpKpYI83dkxhQoWRVJrRaRVZktF/vBqnpUVRPc9RlA\nkIiUxnnaSP/zsZK7L7P3GaWq0aoaHRERcbFhmdzqymegZBWeTh1BkCYx9CsrmjImp2VVJNUpi2MX\nze0xvldVVUSa4ySvA8BhoIaIVMVJFL2A23wZi8kDggvBTf8laNyNjK82mxs2XM+E33dyWwt7qjQm\np2RVJLXjzH0i0klVp3vzxiIyAWgHlBaRWOAFIMh97xFAN2CAiKQAiUAvdX4ypojIg8BMIAAY7dZt\nmIKualto2oe6yz6hd2RT/vXdOtrUKE3lUoX8HZkxBYKcz2O9iCxT1SY+jOeiREdHa0xMjL/DML6U\ndASGteBkcAmaxz1L3cql+fSeFng84u/IjMmTRGSpqkZ7c+75Tp5s/yuNf4UWh07vEnxgPZ/UWsSi\nrQf4bPFZD8PGGB8434Rxv0+iMOZ81OoA9bvRYOsoelVJ4NUZG/jzwHF/R2VMvnfOhCEiXU8tQCV3\n/WoRKZMD8RmTsQ5vICFFeZkRBHuUxyevJC3NWk0Z40vePGHcA3wE3O4uHwJPAgtFpLcPYzMmc4VL\nQ4d/E/zXMsbWW87vfxxk3K/b/R2VMfmaNwkjEKijqreo6i1AXZz5vVvgJA5j/KNBN6hxPVGb/0uP\naim88cMG/th/zN9RGZNveZMwKqvq3nTb+9x9BwGbP9P4jwh0egeRAP4v8EOCA4Qhk1aSakVTxviE\nNwljrohMF5G7ROQuYJq7rzBOJztj/Kd4JbjuZUL+/IWxURuJ2XGI0Qv+8HdUxuRL3iSMgcAYIMpd\nxgED3Tm+r/RlcMZ4pUkfuKQ1jde/RbeaAbw5ayNb9iX4Oypj8h1vpmhVYAHwMzAbmK82iI/JTTwe\nuOl9JPUk/woaQ6EgD4MnrSQlNc3fkRmTr3jTrLYH8DvOUB49gMUi0s3XgRlzXsIvhSufJmTrD3zY\nbBcrdx7mw1+saMqY7ORNkdQzQDNVvUtV78SZ4Og534ZlzAVoORDKRxG99lW61w3j3R83sfGveH9H\nZUy+4U3C8KjqvnTbB7y8zpicFRAInYchiYd4JfRzioUFMujLFZxMsaIpY7KDN1/8P4jITBHpIyJ9\ngO+AGb4Ny5gLVK4+tH6MkHWTGHXZIdbvOcp7szf5Oypj8gVvKr2HAKOAhu4ySlWtw57Jvdo+DqVr\n0WTlS/RuXJLhc7eydMchf0dlTJ7nVdGSqn6lqo+5y1RfB2XMRQkMgc7DIH43z4V8QYUSYQyeuILj\nJ1P8HZkxeVpWU7TGi8jRDJZ4ETmak0Eac94qN4PLBhK8YhyjWsWz4+BxXp2x3t9RGZOnZZowVLWo\nqhbLYCmqqsVyMkhjLsiVz0B4deoueZYHLivDp7/9ydyN+859nTEmQ9bayeRfQWHQ+QM4spNH+Ywa\nZYrwxORVHD5+0t+RGZMn+SxhiMhoEdknImsyOX67iKwSkdUiskhEGqU7tt3dv0JEbM5Vc+EiW8Bl\nAwlcNoZRrRM4eOwkz36d4T9JY8w5+PIJYyzQPovjfwBXqGoD4P9wWmKld6WqRnk716wxmbrqWQiv\nTtWFQxnSrgLTV+1h2srd/o7KmDzHq4QhIpeIyDXuepiIFD3XNao6HziYxfFFqnqqreNvQCVvYjHm\nvAWFOa2mjuzkvhPjiKpcgmenrmb34UR/R2ZMnuLNWFL3AZOBke6uSsDX2RzHPcD36bYV+ElElopI\nv2z+LFMQRbaEywbiWTqa4a0SSElTHv1yhc2dYcx58HZ481bAUQBV3Qxk23zeInIlTsJI3xmwtapG\nAR2AgSLSNovr+4lIjIjExMXFZVdYJj+68hkodSnl5z7Ov26oyuI/DvLBnC3+jsqYPMObhHFCVU83\nKxGRQJwngIsmIg1x5gvvrKoHTu1X1V3u6z5gKs6AhxlS1VGqGq2q0REREdkRlsmvggs5RVOHd9Il\nbhQ3NqrAf2Zvtl7gxnjJm4QxT0SeBsJE5FpgEvDtxX6wiEQCU4Deqrop3f7Cp+pI3Fn9rgOsWYvJ\nHpdcBi0HIDEf8XrUfsoXD+WRL5ZzNMlmGzbmXLxJGEOBOGA1cD/OwIPPnusiEZkA/ArUEpFYEblH\nRPqLSH/3lOeBcOCDM5rPlgUWiMhKnHk4vlPVH87rrzImK1c/D6VrUnjGQ/zv5qrsOZLEM1PXYPOC\nGZM1Odd/EhHpivOlfSJnQrpw0dHRGhNj3TaMF3avgI+uhjo38t+ST/P2T5t5q3sjujW1xnqmYBGR\npd52X/DmCeNGYJOIjBeRTm4dhjF5W4UoaPcUrJ3KwIjltKhaiue/WcMf+4/5OzJjci1vhjfvC1TH\nqbu4FdgqIh/5OjBjfK7VIKjcAs+MIbzfoTRBAR4enrDcJlwyJhPeDm+ejNNP4gtgKdDFl0EZkyMC\nAuHmkaA7Ss1CAAAZkElEQVSplP35Ud7oWp/Vu47w9qyN/o7MmFzJm457HURkLLAZuAWnGWw5H8dl\nTM4oVRXavwbbf6F9/BRuaxHJyPnbmL1+r78jMybX8eYJ406cnt21VLWPqs5QVZuJxuQfjXtDrRtg\n9ku80BzqVSjGo1+u4M8Dx/0dmTG5ijd1GLeq6td5oZWUMRdEBG58D0KLEzKtP8N71QdgwGdLSUpO\n9XNwxuQeWc24t8B9PXPmPZtxz+Q/RSLgpv/B3jVErniHd3tGsXb3UV6cttbfkRmTa2Q1415r9/XM\nmfdsxj2TP9VqD037wKL/cnXwWgZeeSlfLNnJxJid/o7MmFzBm0rv8d7sMyZfuP41iKgNX93HYy2L\nc/ml4Tz39RrW7j7i78iM8TtvKr3rpd9wO+419U04xvhZcCHoPhaSjxMw9T7e79mQEoWCGPDpMo4k\n2nhTpmDLqg7jKRGJBxqmr78A9gLf5FiExuS0MrWh41uw/RdKL32PD25vwu7DiQyeuJI0mz/DFGBZ\n1WG8pqpFgTfPqL8IV9WncjBGY3Je49uh0a0w7w2apq7m6Y51+Gn9XkbM3+rvyIzxG2+a1T4lIiVF\npLmItD215ERwxvhVx7egdA2Ych99GxXihobleXPmRuvUZwosbyq97wXmAzOBl9zXF30bljG5QEgR\npz4j6QgytR9vdq1HvQrFeHjCcjb8ZS3LTcHjTaX3I0AzYIeqXgk0Bg77NCpjcouy9aDDv2HbXAot\nfp+P7mxG4ZBA7hkbw/4E68tqChZvEkaSqiYBiEiIqm4Aavk2LGNykSZ3QoPuMPdVyh2K4aO7ojlw\n7AT9PomxnuCmQPEmYcSKSAmc8aR+FJFvgB2+DcuYXEQEOr0LparB5LtpWOw47/SIYtmfh3lqymqb\nqc8UGN5Uet+sqodV9UXgOeBjbHhzU9CEFIUe4+FEAnx5Bx1rl2TwtTWZunwXH8y1llOmYPCm0rvU\nqQVnXu8FwDl/UonIaBHZJyJrMjkuIvK+iGwRkVUi0iTdsfYistE9NvQ8/h5jfKdsXeg6EnYthemP\n8uCVl9IlqgJvztzI96v3+Ds6Y3zOmyKpZUAcsAlnTow4YLuILBORrHp8jwXaZ3G8A1DDXfoBwwFE\nJAAY5h6vC9wqInW9iNMY36tzozO168rPkcUjeP2WhjSJLMGjE1ewOtaGDzH5mzcJ40ego6qWVtVw\nnC/y6cADwAeZXaSq84GDWbxvZ+ATdfwGlBCR8kBzYIuqblPVkziz/HX27s8xJge0fQJqd4JZzxD6\n53xG9o4mvHAId49bYnNomHzNm4TRUlVnntpQ1VnAZe6XfMhFfHZFIP0woLHuvsz2G5M7eDzO1K4R\ntWFSHyKSdzG2bzOSU9O44+PF7Dua5O8IjfEJbxLGHhF5UkQucZcngL1u0VGaj+M7JxHpJyIxIhIT\nFxfn73BMQRFSBHp97rSgmnAbNUrA2L7N2Z9wgjtH/86R4zZQocl/vEkYtwGVcJrVTgUqu/sCgB4X\n8dm73Pc6pZK7L7P9GVLVUaoararRERERFxGOMeepVFWnJ/j+TTDlfqIqFmNU72i2xR3j7nFLOH7S\nZjI2+Ys3zWr3q+pDQGtVbaKqD6lqnKqeVNUtF/HZ04A73dZSLYEjqroHWALUEJGqIhIM9HLPNSb3\nqdYOrn8VNn4Hc1+ldY3SvNcriuV/HmLAp8s4meL3h3Bjso03zWovF5F1wHp3u5GIZFrZne66CcCv\nQC0RiRWRe0Skv4j0d0+ZAWwDtgAf4lSio6opwIM4Y1atByaqqs2TaXKvFvdD494w/01YOpYODcrz\n6s0NmLcpjsGTbEh0k38EenHOu8D1uL/yVXWlN6PVquqt5ziuwMBMjs3ASSjG5H6neoLH/wXTH4XC\nZejVvCOHjifzxg8bKBEWxMud6yEi/o7UmIviTR0GqnrmpMY2gI4x6QUEQY9xUD4KJveFPxczoN2l\n3N+2GuN/28FbszbaECImz/MmYewUkcsBFZEgEXkct3jKGJNOcGG4fRIUqwCf94C4jQztUJtbm0cy\nbM5WXv9hgyUNk6d5kzD64xQdVcRprRRFJkVJxhR4hUvDHVMgIBjGd0Xi9/CvLvXp3fISRs7bxkvf\nrrOkYfKsc9ZhqOp+4PYciMWY/KFUVbhjMoy5AT69BU/f73m5cz2CAz18vOAPTqam8Urn+ng8Vqdh\n8pZME4aIPJ/Fdaqq/+eDeIzJH8o3gp7j4bPu8MVtyB1TePaGOoQEevhg7lZOpqTxxi0NCbCkYfKQ\nrIqkjmWwANwDPOnjuIzJ+y69Em4eATsWwuS7kdRkhlxfi0evqcnkpbE8+uUKUlKtn4bJOzJ9wlDV\nt0+ti0hRnKla++IMBvh2ZtcZY9Jp0A2OH4Dvn4DJfZFuY3jkmhoEBQr//mEjyalpvNerMcGBXjVY\nNMavsvxX6s6D8QqwCie5NFHVJ1V1X45EZ0x+0OJ+aP8GbJgOk/pAykkeaFed5zrV5fs1f3HvJzHE\nJ9nYUyb3yzRhiMibOMN0xAMNVPVFVT2UY5EZk5+07A8d3nSGEHGTxj2tq/LGLQ1YuGU/3Uf8yp4j\nif6O0pgsZfWEMRioADwL7BaRo+4SLyJHcyY8Y/KRFv2g41tu0rgLUk7Ss1kkY/o0I/ZQIl2GLWTN\nLpuEyeRemSYMVfWoapiqFlXVYumWoqpaLCeDNCbfaH6fmzRmwMQ7IeUEbWtGMHnAZQSI0GPkr8zZ\nYCW+JneymjZjclrz++CGt2HT96eTRu1yxZg6sBVVSxfmnnFLGP/rdn9HacxZLGEY4w/N7oUb3oFN\nP8CEXnAinrLFQpl4/2VcWasMz32zllemr7ORbk2uYgnDGH9pdg90Hgbb5sGYjhD/F4VDAhl1ZzR3\nXXYJHy34g3vGLeHw8ZP+jtQYwBKGMf7V+A647Us4sBU+ugbiNhLgEV7qXJ//61yPBVv2c8P7C1ix\n87C/IzXGEoYxflfjWuj7HaScgI+vhR2LAOh9WRUm9b8cgO4jFjFu0XYbuND4lSUMY3KDCo3h3h+h\ncBn4pDOsnQpAVOUSfPdwa9rUiOCFaWt5aMJyEk7YXOHGPyxhGJNblKwC98yCCk2czn2/DgOgRKFg\nProzmifa12LG6j3c9N8FbPjLukKZnOfThCEi7UVko4hsEZGhGRwfIiIr3GWNiKSKSCn32HYRWe0e\ni/FlnMbkGoVKwZ1fQ52bYObTzpSvKSfweIQH2lXn8/taEn8ihS7DFvLZ4h1WRGVylPjqH5yIBACb\ngGuBWJxhRm5V1XWZnH8j8KiqXuVubwei3fk4vBIdHa0xMZZbTD6QlgazX4SF70HFaGf61+KVANgX\nn8RjX65kwZb9tKlRmtdvaUjFEmH+jdfkWSKyVFWjvTnXl08YzYEtqrpNVU/ijHLbOYvzbwUm+DAe\nY/IOjweufRl6fAJxG2FkW9g6B4AyRUMZf09zXulSn6U7DnH9u/OZ8Puf9rRhfM6XCaMisDPddqy7\n7ywiUghoD3yVbrcCP4nIUhHp57MojcnN6naGfnOgcAR82hXmvwVpaYgId7S8hJmD2tKwUnGemrKa\nu8YsYfdhG8DQ+E5uqfS+EVioqgfT7WutqlFAB2CgiLTN6EIR6SciMSISExcXlxOxGpOzSteAe2dD\nva7w8//BF7dBotMvo3KpQnx6Twv+r3M9YrYf5Pp35/PlEnvaML7hy4SxC6icbruSuy8jvTijOEpV\nd7mv+4CpOEVcZ1HVUaoararRERERFx20MblSSBG45SPo8G/Y8iOMugJ2LwfA4xF6X1aFHx5pS72K\nxXjyq9X0HPUb63ZbSyqTvXyZMJYANUSkqogE4ySFaWeeJCLFgSuAb9LtK+zO8oeIFAauA9b4MFZj\ncj8RZzKmPjMg5SR8eDXMeQ1SncmXIsML8fm9LXn15gZs2ZdAp//+wrNfr+bQMRtaxGQPnyUMVU0B\nHgRmAuuBiaq6VkT6i0j/dKfeDMxS1WPp9pUFFojISuB34DtV/cFXsRqTp0S2gAcWOdO/znsdPrwK\n9jqNDz0e4bYWkcwZ3I47L6vChN930u6tuYxbtN3mDzcXzWfNav3BmtWaAmfdNKevxomjcOXTcPnD\n4Ak4fXjT3nhe+nYtC7ccoFbZorxwU10uv7S0HwM2uc35NKu1hGFMXpcQB989Cuu/hUrNoctwKF39\n9GFVZebavbzy3TpiDyXSrlYEj15Tk0aVS/gxaJNbWMIwpqBRhdWTYcZgp37jiifgsoEQGHL6lKTk\nVMYu2s7IeVs5dDyZq2uXYdA1NWlQqbgfAzf+ZgnDmILq6B6Y8ThsmA6lLoX2r0PN6/5xSsKJFMYt\n2s6o+ds4kpjMtXXLMuiaGtSrYImjILKEYUxBt/kn+OFJOLAFaraH61+F8Ev/cUp8UjJjFm7no1+2\ncTQphevrleX+Ky6lSWRJPwVt/MEShjHGKZpaPBzm/RtST8LlD0GbwRBc+B+nHUlMZvSCPxi98A/i\nk1JoVLkEd7eqQof65QkOzC19e42vWMIwxvzt6B746QVY9SUUqwjthkKjWyEg6B+nHTuRwpRlsYxZ\nuJ1t+49RpmgIvVtewm0tIgkvEpLJm5u8zhKGMeZsf/4GPzwFu5c5c29c8SQ06AEBgf84LS1Nmbc5\njjELtzN/UxzBgR46N6pAj2aVib6kJCLin/iNT1jCMMZkTBU2zYQ5/4K/VjkV4+2GQv1b/tF/45Qt\n++IZs3A7U5btIjE5lUvCC9G1cSW6NqlI5VKF/PAHmOxmCcMYkzVVpyXVnNdg31ooXQvaPQl1u2SY\nOI6dSOH7NX/x1dJYft12AIAWVUtxS5NKdGxYniIhgWddY/IGSxjGGO+kpcH6b2Du6xC3AUpEQvP7\noUlvCM24mW3soeN8vXwXXy3bxR/7jxES6KFNjdJcV7ccV9cpY/UdeYwlDGPM+UlLdZ44fhsBfy6C\n4CIQdRu06H9Wc9xTVJVlfx7m25W7+XHdXnYdTsQj0PSSklxXtxzX1i1LldKFM7zW5B6WMIwxF273\nClg8wuk5npYCNa6Dlv2hajtnJsAMqCrr9hxl1tq9/LhuL+v2OEOrVy9ThNbVS3PZpeG0rBpO8UJB\nGV5v/McShjHm4sXvhZiPIWY0HIuD4pWhYU+nSW66saoysvPgcX5ct5c5G/cRs/0QicmpiED9CsW5\n7NJwLrs0nGZVSlndRy5gCcMYk32Sk5ziqhWfw7Y5oGnOIIeNekH9rhCWdc/wkylprIw9zKItB1i0\ndT/L/zzMydQ0POI8gTSsVIJGlUvQqFJxapcrZp0Fc5glDGOMbxzd43QAXDnBqSQPCIFa7aHOTVDj\n2kwrytNLPJlKzI6DxGw/xKrYw6yMPcJBd5Kn4AAPdSoUo36FYtQoU4QaZYtSo0wRIoqGWP8PH7GE\nYYzxLVXYswJWTIC1U5wiK08QVG0DtTpC7RugWAUv30qJPZTIqtgjrIw9zMqdh1m/5yhHk1JOn1Ms\nNPB08qhSujAVS4RRqWQYFUuGEVHEksnFsIRhjMk5aakQGwMbv4MN3zkDHgJUaAK1OkC1dlCh8VlD\nkWRFVYlLOMGWvQls3pfA5n3xbHbXD54x5WxIoIeKJZzkUbZYKOFFgildOITwIsGEFwkhvHAwpYuE\nUDwsiNAgjyWXM1jCMMb4T9wmp85jw3ewy/3/GFwELrkcqrZ1lrINMm1xdS7xScnsOpzIrkOJxB5K\nTLd+nLj4E+xPOMnJTKaj9QgUDgmkaEgghd2lSEggoUEeAj0eggI9BHmEoAAPQYFCYLoY01RRdV9x\nHrLAmWr9VApy1p2tlLQ0TqSkkZyqJKekcTI1jWR3cWKR0+enz2FJyakkJqeSeDKVpOS00+snU9MQ\n9zrE+Vs84nxaRNEQ5g658oLu5/kkDJ82URCR9sB7QADwkaq+fsbxdsA3wB/urimq+rI31xpjcqmI\nmhDxGLR5DI4dgB0L4I/5sG0ebJ7lnBNWEiIvh0pNoWJT5wnEi/oPgKKhQdQuF0TtcsUyPK6qxJ9I\n4UDCSQ4kOAnkwLETHE1M4diJFBLcJf36oePOF3lKqp7+Yj+1Dn9/uZ/6ghZ320ka6n4ubiJxtoMC\nPAQFeAgO9BDsJqDgAM/pJJRK2lkJSATCggIoFhpEaHAAYUHuEhxAUICc/ow0dyVNlTSFwsFn9873\nBZ89YYhIALAJuBaIBZYAt6rqunTntAMeV9VO53ttRuwJw5hc7uhu+OMX+GOeMxjiwa1/Hytd00ke\nFZtC+SiIqAWhGScFk31yyxNGc2CLqm5zg/oC6Axk+aWfDdcaY3KrYhWgUU9nATh+EHYvh13LYNdS\n2DLbaYF1+vxKUKY2RNSGMnWc19I1vH4aMdnLlwmjIrAz3XYs0CKD8y4XkVXALpynjbXnca0xJi8r\nVAqqX+0s4JTLHImFv1ZD3HrYt8F5/eMXSD3x93WhxaHEJVDyEve1ivNavCIUKQthpS64jsRkzt/d\nLJcBkaqaICIdga+BGufzBiLSD+gHEBkZmf0RGmNyjgiUqOwstTv+vT8tFQ5th33rnWKsQzvg8A4n\noWya9c9kAiABUKSMu5R1XguFO4kmtDiElki3XhyCwiAwDIJCndcAf381ppOW5syYmOY2MxYBxK1h\n9/y9fh6t0C6UL+/KLqByuu1K7r7TVPVouvUZIvKBiJT25tp0140CRoFTh5E9oRtjchVPgDMIYkYD\nIaalQcJeJ4HE74GEfc52wt6/1/esgqTDkJLk3edJgJtEQsAT6GyLx1k8nr/XT7WP+kdTXXdd00BT\nnWSn6qxrmrOkPy99AlB1kkNqsvt60rnuXAqXgSGbvfvbLoIvE8YSoIaIVMX5su8F3Jb+BBEpB+xV\nVRWR5oAHOAAcPte1xhgDOF/gxco7y7kkJ8GJo5B0xF0OQ+JhSE50kklyIqScgJRE59yURPcLP+3s\nL/20U1/k6X6nnm5EpH8nGU9AuvV0SebUear/fI+AEAgIdp4YAoL/Xj81T8mp80+/pkFQzowK7LOE\noaopIvIgMBOnaexoVV0rIv3d4yOAbsAAEUkBEoFe6jTbyvBaX8VqjCkggkKdpUgZf0eSJ1nHPWOM\nKcDOp1mtNSMwxhjjFUsYxhhjvGIJwxhjjFcsYRhjjPGKJQxjjDFesYRhjDHGK5YwjDHGeCVf9cMQ\nkThgxwVeXhrYn43h5Ad2T85m9+Rsdk/OlpfuySWqGuHNifkqYVwMEYnxtvNKQWH35Gx2T85m9+Rs\n+fWeWJGUMcYYr1jCMMYY4xVLGH8b5e8AciG7J2eze3I2uydny5f3xOowjDHGeMWeMIwxxnilwCcM\nEWkvIhtFZIuIDPV3PP4iIqNFZJ+IrEm3r5SI/Cgim93Xkv6MMaeJSGURmSMi60RkrYg84u4vsPdF\nREJF5HcRWenek5fc/QX2npwiIgEislxEprvb+e6eFOiEISIBwDCgA1AXuFVE6vo3Kr8ZC7Q/Y99Q\nYLaq1gBmu9sFSQowWFXrAi2Bge6/j4J8X04AV6lqIyAKaC8iLSnY9+SUR4D16bbz3T0p0AkDaA5s\nUdVtqnoS+ALo7OeY/EJV5wMHz9jdGRjnro8DuuRoUH6mqntUdZm7Ho/zZVCRAnxf1JHgbga5i1KA\n7wmAiFQCbgA+Src7392Tgp4wKgI7023HuvuMo6yq7nHX/wLK+jMYfxKRKkBjYDEF/L64RS8rgH3A\nj6pa4O8J8B/gCSAt3b58d08KesIwXnLnWi+QTepEpAjwFTBIVY+mP1YQ74uqpqpqFFAJaC4i9c84\nXqDuiYh0Avap6tLMzskv96SgJ4xdQOV025XcfcaxV0TKA7iv+/wcT44TkSCcZPGZqk5xdxf4+wKg\nqoeBOTh1XwX5nrQCbhKR7TjF2leJyKfkw3tS0BPGEqCGiFQVkWCgFzDNzzHlJtOAu9z1u4Bv/BhL\njhMRAT4G1qvqO+kOFdj7IiIRIlLCXQ8DrgU2UIDviao+paqVVLUKznfIz6p6B/nwnhT4jnsi0hGn\n/DEAGK2q//JzSH4hIhOAdjijbO4FXgC+BiYCkTijAPdQ1TMrxvMtEWkN/AKs5u+y6adx6jEK5H0R\nkYY4FbgBOD84J6rqyyISTgG9J+mJSDvgcVXtlB/vSYFPGMYYY7xT0IukjDHGeMkShjHGGK9YwjDG\nGOMVSxjGGGO8YgnDGGOMVyxhGJMLiEi7U6OcGpNbWcIwxhjjFUsYxpwHEbnDnQ9ihYiMdAfiSxCR\nd935IWaLSIR7bpSI/CYiq0Rk6qn5EESkuoj85M4psUxELnXfvoiITBaRDSLymdvT3JhcwxKGMV4S\nkTpAT6CVO/heKnA7UBiIUdV6wDycXvIAnwBPqmpDnN7ip/Z/Bgxz55S4HDg1omljYBDO3CzVcMYo\nMibXCPR3AMbkIVcDTYEl7o//MJwB5dKAL91zPgWmiEhxoISqznP3jwMmiUhRoKKqTgVQ1SQA9/1+\nV9VYd3sFUAVY4Ps/yxjvWMIwxnsCjFPVp/6xU+S5M8670PF2TqRbT8X+f5pcxoqkjPHebKCbiJSB\n03M2X4Lz/6ibe85twAJVPQIcEpE27v7ewDx35r5YEenivkeIiBTK0b/CmAtkv2CM8ZKqrhORZ4FZ\nIuIBkoGBwDGciYSexSmi6ulechcwwk0I24C+7v7ewEgRedl9j+45+GcYc8FstFpjLpKIJKhqEX/H\nYYyvWZGUMcYYr9gThjHGGK/YE4YxxhivWMIwxhjjFUsYxhhjvGIJwxhjjFcsYRhjjPGKJQxjjDFe\n+X9bILL7SuZFxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa7d7400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_i,g_train_loss,label='train_loss')\n",
    "plt.plot(g_i,g_valid_loss,label='valid_loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Negative log-likelihood\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xb28abe0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFAoiIlLh/wPi8dOB+qw/HgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb01d908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_i,g_train_acc,label='train_acc')\n",
    "plt.plot(g_i,g_valid_acc,label='valid_acc')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([0.,1.])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
